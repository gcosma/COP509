{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gcosma/COP509/blob/main/LabSolutions/Lab_Exercise_(No_4a).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFZyJ3CTsFZ5"
      },
      "source": [
        "\n",
        "# **Lab Exercise (No. 4a)**\n",
        "\n",
        "**Task Overview**\n",
        "\n",
        "1. Your task is to use the cleaned ArtReviews dataset and the ArtRatings(classes) and repeat the Lab sheet to apply LSA.\n",
        "\n",
        "2. Write code to retrieve the top 10 results for the 2 given queries.\n",
        "['I really enjoy these scissors!',\n",
        "'I hate the pen!'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***pre-work1: mount drive***"
      ],
      "metadata": {
        "id": "kHRD-Ndy2Gss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYs5fOJFpivJ",
        "outputId": "9a3bd889-6821-485a-c39a-baac70957a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***pre-work2: set datasets***"
      ],
      "metadata": {
        "id": "k2vWpStmklmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/My Drive/Colab Notebooks/21COP509/LabDatasets/\"\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/21COP509/LabDatasets/\"\n",
        "\n",
        "datasets = 'Arts5000' # ArtsAll, Arts5000\n",
        "\n",
        "\n",
        "if datasets == 'ArtsAll':\n",
        "  # Arts all\n",
        "  data_all_reviews = 'ArtsReviews.txt'\n",
        "  data_all_ratings = 'ArtsRatings.txt'\n",
        "  data_train_reviews = 'ArtsReviews_train.txt'\n",
        "  data_test_reviews = 'ArtsReviews_test.txt'\n",
        "  data_train_ratings = 'ArtsRatings_train.txt'\n",
        "  data_test_ratings = 'ArtsRatings_test.txt'\n",
        "else:\n",
        "  # Arts 5000\n",
        "  data_all_reviews = 'Reduced_ArtsReviews_5000.txt'\n",
        "  data_all_ratings = 'Reduced_ArtsRatings_5000.txt'\n",
        "  data_train_reviews = 'ArtsReviews_5000_train.txt'\n",
        "  data_test_reviews = 'ArtsReviews_5000_test.txt'\n",
        "  data_train_ratings = 'ArtsRatings_5000_train.txt'\n",
        "  data_test_ratings = 'ArtsRatings_5000_test.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN1UEcu9kinI",
        "outputId": "ecbd903d-ae12-4862-ee21-4e33eae34c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ArtsRatings_5000_test.txt   ArtsReviews_5000_train.txt\tReduced_ArtsRatings_5000.txt\n",
            "ArtsRatings_5000_train.txt  glove.6B.100d.txt\t\tReduced_ArtsReviews_5000.txt\n",
            "ArtsReviews_5000_test.txt   metamorphosis_clean.txt\treview_polarity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.fit the LSA model**"
      ],
      "metadata": {
        "id": "VHfaf9ly2A_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "from pandas import DataFrame\n",
        "from matplotlib import pyplot\n",
        "import random\n",
        "import nltk\n",
        "import numpy\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import rand\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from numpy import argsort\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\t# load doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# update counts\n",
        "\tvocab.update(tokens)\n",
        "\n",
        " # load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "\t# convert lines to a single blob of text\n",
        "\tdata = '\\n'.join(lines)\n",
        "\t# open file\n",
        "\tfile = open(filename, 'w')\n",
        "\t# write text\n",
        "\tfile.write(data)\n",
        "\t# close file\n",
        "\tfile.close()\n",
        "\n",
        "# split train and test\n",
        "def random_sample(num1, num2):\n",
        "    dataList = list(range(num1))\n",
        "    TrainIndex = []\n",
        "    for i in range(num2):\n",
        "        randIndex = int(random.uniform(0,len(dataList)))\n",
        "        TrainIndex.append(dataList[randIndex])\n",
        "        del(dataList[randIndex])\n",
        "    TestIndex = dataList\n",
        "    return TrainIndex,TestIndex\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc_lines(filename):\n",
        "\t file = open(filename,'rt')\n",
        "\t lines = list()\n",
        "   # read all text as lines\n",
        "\t while 1:\n",
        "\n",
        "\t\t line = file.readline()\n",
        "\t\t if not line:\n",
        "\t\t   break\n",
        "\t\t pass\n",
        "\t\t lines.append(line.strip(\"\\n\"))\n",
        "   # close the file\n",
        "\t file.close()\n",
        "\t return lines\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\n",
        "\t# convert to lower case\n",
        "\ttokens = [word.lower() for word in tokens]\n",
        "\n",
        " \t# stemming of words\n",
        "\tfrom nltk.stem.porter import PorterStemmer\n",
        "\tporter = PorterStemmer()\n",
        "\ttokens = [porter.stem(word) for word in tokens]\n",
        "\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(doc):\n",
        "\t# load the doc\n",
        "\t# doc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\t# tokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(files):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor doc in files:\n",
        "\t\t# print(len(doc))\n",
        "\t\tline = doc_to_line(doc)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        "\n",
        "# prepare words encoding of docs\n",
        "def prepare_data(train_docs, mode, vocab):\n",
        "\t# encode training data set\n",
        "\tvectorizer = CountVectorizer(vocabulary=vocab)\n",
        "\ttransformer = TfidfTransformer(norm='l2')\n",
        "\tXtrain = transformer.fit_transform(vectorizer.fit_transform(train_docs))\n",
        "\treturn Xtrain\n",
        "\n",
        "# load the dataset #########################################\n",
        "ArRe_train_lines = load_doc_lines(data_path + data_all_reviews)\n",
        "\n",
        "train_docs = process_docs(ArRe_train_lines)\n",
        "\n",
        "# generate vocabulary\n",
        "vocab = []\n",
        "for ll in train_docs:\n",
        "  tt = ll.split()\n",
        "  for ww in tt:\n",
        "    if ww not in vocab:\n",
        "      vocab.append(ww)\n",
        "# print(vocab)\n",
        "# print(len(vocab))\n",
        "\n",
        "Xtrain = prepare_data(train_docs, 'tfidf', vocab)\n",
        "\n",
        "trunc_SVD_model = TruncatedSVD(n_components=50)\n",
        "approx_Xtrain = trunc_SVD_model.fit_transform(Xtrain)\n",
        "print(\"Approximated Xtrain shape: \" + str(approx_Xtrain.shape))\n"
      ],
      "metadata": {
        "id": "ivEseKDbryVZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ded9ba-5b71-4446-cbf4-9230e7fc82d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approximated Xtrain shape: (5000, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.retrieval**"
      ],
      "metadata": {
        "id": "_8Am4FUx19vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess query\n",
        "def preprocess_query(review, mode, vocab):\n",
        "\t# clean\n",
        "\ttokens = clean_doc(review)\n",
        "\t# convert to line\n",
        "\tline = ' '.join(tokens)\n",
        "\t# encode\n",
        "\tvectorizer = CountVectorizer(vocabulary=vocab)\n",
        "\ttransformer = TfidfTransformer(norm='l2')\n",
        "\tencoded = transformer.fit_transform(vectorizer.fit_transform([line]))\n",
        "\treturn encoded\n",
        "\n",
        "querys = ['I really enjoy these scissors!',\n",
        "          'I hate the pen!'\n",
        "]\n",
        "for query in querys:\n",
        "  # retrieval\n",
        "  encoded_query = preprocess_query(query, 'tfidf', vocab)\n",
        "  # print(encoded_query.shape)\n",
        "\n",
        "  transformed_query = trunc_SVD_model.transform(encoded_query)\n",
        "  # print(\"Transformed query: \" + str(transformed_query))\n",
        "  # print(\"Query shape: \" + str(transformed_query.shape))\n",
        "\n",
        "  # print(type(approx_Xtrain))\n",
        "  # print(approx_Xtrain)\n",
        "  similarities = cosine_similarity(approx_Xtrain, transformed_query)\n",
        "  # print(\"Similarities shape: \" + str(similarities.shape))\n",
        "\n",
        "  Top_n_reviews=10\n",
        "  # indexes = np.argsort(similarities.flat)[-Top_n_reviews:]\n",
        "  indexes = np.argsort(similarities.flat)[::-1]\n",
        "\n",
        "  print('\\n' + 'Query: ' + query)\n",
        "  for i in range(Top_n_reviews):\n",
        "    print(\"Top \" + str(i+1) + ' result:')\n",
        "    print(\"Reviews ID: \" + str(indexes[i]))\n",
        "    print(ArRe_train_lines[indexes[i]])\n",
        "    # print(\"similarities: \" + str(similarities.flat[indexes[i]]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGJweykI0xI4",
        "outputId": "83bbb465-5cee-4bbb-a812-4d2c56ef5e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: I really enjoy these scissors!\n",
            "Top 1 result:\n",
            "Reviews ID: 4332\n",
            "I really don't like. The price is really cheap, but it really worth &#34;every penny&#34;. So be prepare what you pay is what you gonna get!\n",
            "Top 2 result:\n",
            "Reviews ID: 699\n",
            "If you are into serious hobbies, or anything that requires really high quality wood, I would not recommend this bag at all.It worked okay for what I needed, as I was just cutting up some wood to practice with, but for actual use, it's not really ideal.There were a lot of really large chunks of wood, that you really couldn't do a lot of stuff with. If you're looking for a lot of thin slices, this really isn't your bad. Additionally, I know that balsa wood is generally fragile, but some of the thin strips had already broken by the time I received my package.Overall, this wood wasn't really that high quality. It was very rough, and I've seen a lot better for a little bit higher prices.\n",
            "Top 3 result:\n",
            "Reviews ID: 1986\n",
            "I haven't really used it yet, but I am sure it will come in handy. I do plan to apply to doing my crafts.\n",
            "Top 4 result:\n",
            "Reviews ID: 2881\n",
            "haven't really had a need to use it yet - but feel it is notquite as sturdy as ad depicted\n",
            "Top 5 result:\n",
            "Reviews ID: 3583\n",
            "the scissors aren't very good, but you can cut a thread and i guess that's really all they are meant to do. the size and convenience are what i really like about this kit.\n",
            "Top 6 result:\n",
            "Reviews ID: 1512\n",
            "I really like these snips that I use on my boats bait and rigging station. Once you get used to the lock system, they work really well and cut braid, dacron and mono cleanly!\n",
            "Top 7 result:\n",
            "Reviews ID: 603\n",
            "Really like these brushes, however I did cut the bristles on two of them to make them more to my liking/\n",
            "Top 8 result:\n",
            "Reviews ID: 897\n",
            "This stuff works really well on my jewelry. Everything comes out sparkly and very bright, it comes with a little brush and a basket for dipping. It doesn't smell bad like some other cleaners. I am really happy with this.\n",
            "Top 9 result:\n",
            "Reviews ID: 1270\n",
            "If you have a Paasche with a plastic handle...I got this for my VL...this is a really nice addition. Spend the 7 bucks and get it.\n",
            "Top 10 result:\n",
            "Reviews ID: 3336\n",
            "Very disagppointed with this booklet. It only lists the Energizer Brand of batteries, so it's not really a reference.\n",
            "\n",
            "Query: I hate the pen!\n",
            "Top 1 result:\n",
            "Reviews ID: 1900\n",
            "The pen itself was visibly as depicted: a Murano-style glass pen, with all the slight irregularities that should be expected of hand-shaped glass. I could not rate how the ink flowed from the pen, as the pen had been very poorly packaged and had arrived broken in two places. Most disappointing, as this pen was intended to be given as a gift.\n",
            "Top 2 result:\n",
            "Reviews ID: 570\n",
            "The package sucks as a pen holder except if you're traveling and do not want the pens rolling around loose anywhere.The pens are awesome but if you're heavy handed you NEED to be careful, the smallest nibswear down fast.\n",
            "Top 3 result:\n",
            "Reviews ID: 1345\n",
            "I'll start by saying I don't use this pen for drafting. I love to write with this pen because the lines get really dark, are extremely consistent, and the feel of writing with it is very satisfying, like a sharp pencil or a fountain pen.These pens do require you to keep it at a roughly 80-90 degree angle to maintain ink flow, but I don't mind that.I also don't like to use the Koh-I-Noor Technical Pen Ink. It clogs the pen quickly and regularly. Black India Fountain Pen Ink almost never clogs it, and is still very dark and doesn't soak through paper.With minor maintenance and the right ink, this pen will last you forever.\n",
            "Top 4 result:\n",
            "Reviews ID: 2589\n",
            "I haven't given them a real work out, but it's hard to tell the difference between some of the pen weights. Next time I will purchase individual pens and skip some of the intermediate sizes.\n",
            "Top 5 result:\n",
            "Reviews ID: 3924\n",
            "This Pen is Not Good. This Was My Very First Fountain Pen And iWas Very Very Disappointed With How The Pen Has A Problem With The Ink Flow And You Are Planing To Buy This Pen I Wouldn't Recommend It To Any Body.\n",
            "Top 6 result:\n",
            "Reviews ID: 4396\n",
            "I might have given these pens a 3 star rating but one pen came without any ink. A decent pen at a decent price but nothing that would inspire me to purchase them again. For their purpose when they come with ink work as stated.\n",
            "Top 7 result:\n",
            "Reviews ID: 958\n",
            "I received this pen as a gift. I have not written with a fountain pen because I thought it might be messy, leaving ink on my fingers. But this pen is great and no ink splotches on my hands. I would definitely give this as a gift.\n",
            "Top 8 result:\n",
            "Reviews ID: 2338\n",
            "These are my &#34;go to&#34; pens for drawing and painting! I love them because of their permanent black ink, their beautiful variety in nib sizes and their long lives. Whenever I think I might be running low on ink, I'm here on Amazon reordering so I don't suffer pen withdrawal. These pens are made for more than just doodling. They are often the backbone of my art. Thanks so much!\n",
            "Top 9 result:\n",
            "Reviews ID: 1087\n",
            "THESE PENS ARE A HARD TO FIND ITEM IN LOCAL STORES. THEY ARE ONE OF THE BEST WRITING PENS I'VE EVER USED.\n",
            "Top 10 result:\n",
            "Reviews ID: 4677\n",
            "These were the very first inking pens I ever got, and I was shocked by how well they worked.First off, they come in a variety of sizes, from the fairly thick .05 to the deliciously thin .005. It offers a nice range of line widths for artwork and a must-have for comic art. (And be sure to get the Brush Pen, the pen that acts like a mini-paintbrush!)The peeve I have about Sakura Micron is that the tip of the .005 is a bit fragile and liable to bend, but considering how tiny it is, you sort of expect that.Overall, a good pen whose sizes and alternate colors really make them stand out, but you have to be careful with the points, especially .005.\n"
          ]
        }
      ]
    }
  ]
}