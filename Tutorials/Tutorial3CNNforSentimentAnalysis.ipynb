{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gcosma/COP509/blob/main/Tutorial3CNNforSentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUh65QeGaZq_"
      },
      "source": [
        "#**Deep Convolutional Neural Network for Sentiment Analysis (Text Classification)**\n",
        "\n",
        "https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/\n",
        "\n",
        "Develop a Deep Learning Model to Automatically Classify Movie Reviews\n",
        "as Positive or Negative in Python with Keras, Step-by-Step.\n",
        "\n",
        "Word embeddings are a technique for representing text where different words with similar meaning have a similar real-valued vector representation.\n",
        "\n",
        "They are a key breakthrough that has led to great performance of neural network models on a suite of challenging natural language processing problems.\n",
        "\n",
        "In this tutorial, you will discover how to develop word embedding models for neural networks to classify movie reviews.\n",
        "\n",
        "After completing this tutorial, you will know:\n",
        "\n",
        "- How to prepare movie review text data for classification with deep learning methods.\n",
        "- How to learn a word embedding as part of fitting a deep learning model.\n",
        "- How to learn a standalone word embedding and how to use a pre-trained embedding in a neural network model.\n",
        "\n",
        "Kick-start your project with my new book Deep Learning for Natural Language Processing, including step-by-step tutorials and the Python source code files for all examples.\n",
        "\n",
        "Let’s get started.\n",
        "\n",
        "Update Nov/2019: Fixed code typo when preparing training dataset (thanks HSA).\n",
        "Update Aug/2020: Updated link to movie review dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKdbUKeQai6v"
      },
      "source": [
        "**Tutorial Overview**\n",
        "\n",
        "This tutorial is divided into 5 parts; they are:\n",
        "\n",
        "1. Movie Review Dataset\n",
        "2. Data Preparation\n",
        "3. Train Embedding Layer\n",
        "4. Train word2vec Embedding\n",
        "5. Use Pre-trained Embedding\n",
        "\n",
        "**Python Environment**\n",
        "\n",
        "This tutorial assumes you have a Python SciPy environment installed, ideally with Python 3.\n",
        "\n",
        "You must have Keras (2.2 or higher) installed with either the TensorFlow or Theano backend.\n",
        "\n",
        "The tutorial also assumes you have scikit-learn, Pandas, NumPy, and Matplotlib installed.\n",
        "\n",
        "If you need help with your environment, see this tutorial:\n",
        "\n",
        "- How to Setup a Python Environment for Machine Learning and Deep Learning with Anaconda\n",
        "A GPU is not required for this tutorial, nevertheless, you can access GPUs cheaply on Amazon Web Services. Learn how in this tutorial:\n",
        "\n",
        "- How to Setup Amazon AWS EC2 GPUs to Train Keras Deep Learning Models (step-by-step)\n",
        "Let’s dive in.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ7VZYluaoGu"
      },
      "source": [
        "#**1. Movie Review Dataset**\n",
        "\n",
        "The Movie Review Data is a collection of movie reviews retrieved from the imdb.com website in the early 2000s by Bo Pang and Lillian Lee. The reviews were collected and made available as part of their research on natural language processing.\n",
        "\n",
        "The data contains 1000 positive and 1000 negative reviews all written before 2002, with a cap of 20 reviews per author (312 authors total) per category.The authors refer to this corpus as the polarity dataset.\"\n",
        "\n",
        "You can download the dataset from here:\n",
        "\n",
        "- Movie Review Polarity Dataset (review_polarity.tar.gz, 3MB) https://raw.githubusercontent.com/jbrownlee/Datasets/master/review_polarity.tar.gz\n",
        "\n",
        "After unzipping the file, you will have a directory called “txt_sentoken” with two sub-directories containing the text “neg” and “pos” for negative and positive reviews. Reviews are stored one per file with a naming convention cv000 to cv999 for each neg and pos.\n",
        "\n",
        "Next, let’s look at loading and preparing the text data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKI7C2c5JuPG"
      },
      "source": [
        "#**2. Data Preparation**\n",
        "\n",
        "In this section, we will look at 3 things:\n",
        "\n",
        "1. Separation of data into training and test sets.\n",
        "2. Loading and cleaning the data to remove punctuation and numbers.\n",
        "3. Defining a vocabulary of preferred words.\n",
        "\n",
        "##**Split into Train and Test Sets**\n",
        "\n",
        "We are pretending that we are developing a system that can predict the sentiment of a textual movie review as either positive or negative.\n",
        "\n",
        "This means that after the model is developed, we will need to make predictions on new textual reviews. This will require all of the same data preparation to be performed on those new reviews as is performed on the training data for the model.\n",
        "\n",
        "We will ensure that this constraint is built into the evaluation of our models by splitting the training and test datasets prior to any data preparation. This means that any knowledge in the data in the test set that could help us better prepare the data (e.g. the words used) are unavailable in the preparation of data used for training the model.\n",
        "\n",
        "That being said, we will use the last 100 positive reviews and the last 100 negative reviews as a test set (100 reviews) and the remaining 1,800 reviews as the training dataset.\n",
        "\n",
        "This is a 90% train, 10% split of the data.\n",
        "\n",
        "The split can be imposed easily by using the filenames of the reviews where reviews named 000 to 899 are for training data and reviews named 900 onwards are for test.\n",
        "\n",
        "##**Loading and Cleaning Reviews**\n",
        "\n",
        "The text data is already pretty clean; not much preparation is required.\n",
        "\n",
        "If you are new to cleaning text data, see this post:\n",
        "\n",
        "How to Clean Text for Machine Learning with Python\n",
        "Without getting bogged down too much in the details, we will prepare the data using the following way:\n",
        "\n",
        "Split tokens on white space.\n",
        "- Remove all punctuation from words.\n",
        "- Remove all words that are not purely comprised of alphabetical characters.\n",
        "- Remove all words that are known stop words.\n",
        "- Remove all words that have a length <= 1 character.\n",
        "\n",
        "We can put all of these steps into a function called clean_doc() that takes as an argument the raw text loaded from a file and returns a list of cleaned tokens. We can also define a function load_doc() that loads a document from file ready for use with the clean_doc() function.\n",
        "\n",
        "An example of cleaning the first positive review is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRO54moURQCY",
        "outputId": "e76a965f-0692-46ce-a153-1fd902017de5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNMhFbDbVUA_",
        "outputId": "cc136f49-14d0-446a-d1b5-840a72f9ead8"
      },
      "source": [
        "data_path = \"/content/drive/My Drive/Colab Notebooks/21COP509/LabDatasets/review_polarity/\"\n",
        "data_path2 = \"/content/drive/My Drive/Colab Notebooks/21COP509/LabDatasets/\"\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/21COP509/LabDatasets/review_polarity/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "poldata.README.2.0  txt_sentoken\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bSZI6clalD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ef4b11-88e0-484e-834a-fcaca022790c"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO57R4sQyh4J",
        "outputId": "5815f814-cd9f-4bd2-a6c1-1ce249f488ad"
      },
      "source": [
        "# load the document\n",
        "filename = data_path + 'txt_sentoken/pos/cv000_29590.txt'\n",
        "text = load_doc(filename)\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrltnVixavzv"
      },
      "source": [
        "Running the example prints a long list of clean tokens.\n",
        "\n",
        "There are many more cleaning steps we may want to explore and I leave them as further exercises.\n",
        "\n",
        "I’d love to see what you can come up with.\n",
        "Post your approaches and findings in the comments at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcDfKhOrdUrd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55bcc551-e814-45df-afb7-27653a3dc0bd"
      },
      "source": [
        "['creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['creepy',\n",
              " 'place',\n",
              " 'even',\n",
              " 'acting',\n",
              " 'hell',\n",
              " 'solid',\n",
              " 'dreamy',\n",
              " 'depp',\n",
              " 'turning',\n",
              " 'typically',\n",
              " 'strong',\n",
              " 'performance',\n",
              " 'deftly',\n",
              " 'handling',\n",
              " 'british',\n",
              " 'accent',\n",
              " 'ians',\n",
              " 'holm',\n",
              " 'joe',\n",
              " 'goulds',\n",
              " 'secret',\n",
              " 'richardson',\n",
              " 'dalmatians',\n",
              " 'log',\n",
              " 'great',\n",
              " 'supporting',\n",
              " 'roles',\n",
              " 'big',\n",
              " 'surprise',\n",
              " 'graham',\n",
              " 'cringed',\n",
              " 'first',\n",
              " 'time',\n",
              " 'opened',\n",
              " 'mouth',\n",
              " 'imagining',\n",
              " 'attempt',\n",
              " 'irish',\n",
              " 'accent',\n",
              " 'actually',\n",
              " 'wasnt',\n",
              " 'half',\n",
              " 'bad',\n",
              " 'film',\n",
              " 'however',\n",
              " 'good',\n",
              " 'strong',\n",
              " 'violencegore',\n",
              " 'sexuality',\n",
              " 'language',\n",
              " 'drug',\n",
              " 'content']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3fbkkU6a05-"
      },
      "source": [
        "**Define a Vocabulary**\n",
        "\n",
        "It is important to define a vocabulary of known words when using a bag-of-words or embedding model.\n",
        "\n",
        "The more words, the larger the representation of documents, therefore it is important to constrain the words to only those believed to be predictive. This is difficult to know beforehand and often it is important to test different hypotheses about how to construct a useful vocabulary.\n",
        "\n",
        "We have already seen how we can remove punctuation and numbers from the vocabulary in the previous section. We can repeat this for all documents and build a set of all known words.\n",
        "\n",
        "We can develop a vocabulary as a Counter, which is a dictionary mapping of words and their counts that allow us to easily update and query.\n",
        "\n",
        "Each document can be added to the counter (a new function called add_doc_to_vocab()) and we can step over all of the reviews in the negative directory and then the positive directory (a new function called process_docs()).\n",
        "\n",
        "The complete example is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncGY7M7qazBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "879c2907-06f6-477a-c04d-ae1209b79519"
      },
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\t# load doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# update counts\n",
        "\tvocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_trian and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# add doc to vocab\n",
        "\t\tadd_doc_to_vocab(path, vocab)\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs(data_path + 'txt_sentoken/neg', vocab, True)\n",
        "process_docs(data_path + 'txt_sentoken/pos', vocab, True)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44276\n",
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('bad', 1248), ('could', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDcLJ1IBa9Am"
      },
      "source": [
        "Running the example shows that we have a vocabulary of 44,276 words.\n",
        "\n",
        "We also can see a sample of the top 50 most used words in the movie reviews.\n",
        "\n",
        "Note, that this vocabulary was constructed based on only those reviews in the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezFVtEPqddA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4094e8-f4f9-4dc3-dc5e-4bbbed211a56"
      },
      "source": [
        "44276\n",
        "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('film', 7983),\n",
              " ('one', 4946),\n",
              " ('movie', 4826),\n",
              " ('like', 3201),\n",
              " ('even', 2262),\n",
              " ('good', 2080),\n",
              " ('time', 2041),\n",
              " ('story', 1907),\n",
              " ('films', 1873),\n",
              " ('would', 1844),\n",
              " ('much', 1824),\n",
              " ('also', 1757),\n",
              " ('characters', 1735),\n",
              " ('get', 1724),\n",
              " ('character', 1703),\n",
              " ('two', 1643),\n",
              " ('first', 1588),\n",
              " ('see', 1557),\n",
              " ('way', 1515),\n",
              " ('well', 1511),\n",
              " ('make', 1418),\n",
              " ('really', 1407),\n",
              " ('little', 1351),\n",
              " ('life', 1334),\n",
              " ('plot', 1288),\n",
              " ('people', 1269),\n",
              " ('could', 1248),\n",
              " ('bad', 1248),\n",
              " ('scene', 1241),\n",
              " ('movies', 1238),\n",
              " ('never', 1201),\n",
              " ('best', 1179),\n",
              " ('new', 1140),\n",
              " ('scenes', 1135),\n",
              " ('man', 1131),\n",
              " ('many', 1130),\n",
              " ('doesnt', 1118),\n",
              " ('know', 1092),\n",
              " ('dont', 1086),\n",
              " ('hes', 1024),\n",
              " ('great', 1014),\n",
              " ('another', 992),\n",
              " ('action', 985),\n",
              " ('love', 977),\n",
              " ('us', 967),\n",
              " ('go', 952),\n",
              " ('director', 948),\n",
              " ('end', 946),\n",
              " ('something', 945),\n",
              " ('still', 936)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff5or85UbC6X"
      },
      "source": [
        "We can step through the vocabulary and remove all words that have a low occurrence, such as only being used once or twice in all reviews.\n",
        "\n",
        "For example, the following snippet will retrieve only the tokens that of appears 2 or more times in all reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_1M5hdDbDs3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c140e3e7-9a63-4a5e-f3ec-c161bb4caae1"
      },
      "source": [
        "# keep tokens with a min occurrence\n",
        "min_occurane = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l97Ej4clbG_X"
      },
      "source": [
        "Running the above example with this addition shows that the vocabulary size drops by a little more than half its size from 44,276 to 25,767 words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fJv-gJHdvr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae84cd0-07c5-4720-db72-bbf1054804bb"
      },
      "source": [
        "25767"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25767"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKDJ5_D0bJXv"
      },
      "source": [
        "Finally, the vocabulary can be saved to a new file called vocab.txt that we can later load and use to filter movie reviews prior to encoding them for modeling. We define a new function called save_list() that saves the vocabulary to file, with one word per file.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvOZ_RzFbLRf"
      },
      "source": [
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "\t# convert lines to a single blob of text\n",
        "\tdata = '\\n'.join(lines)\n",
        "\t# open file\n",
        "\tfile = open(filename, 'w')\n",
        "\t# write text\n",
        "\tfile.write(data)\n",
        "\t# close file\n",
        "\tfile.close()\n",
        "\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jNEfPmLbNMn"
      },
      "source": [
        "Running the min occurrence filter on the vocabulary and saving it to file, you should now have a new file called vocab.txt with only the words we are interested in.\n",
        "\n",
        "The order of words in your file will differ, but should look something like the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCRZJFz8d3GM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "deb1dbd7-db8e-456c-a288-7f12ee44b835"
      },
      "source": [
        "aberdeen\n",
        "dupe\n",
        "burt\n",
        "libido\n",
        "hamlet\n",
        "arlene\n",
        "available\n",
        "corners\n",
        "web\n",
        "columbia\n",
        "..."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'aberdeen' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c612eb94f5e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maberdeen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdupe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mburt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlibido\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhamlet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'aberdeen' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DW_-P1Ad5G9"
      },
      "source": [
        "We are now ready to look at learning features from the reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq-sTkgxbRo2"
      },
      "source": [
        "#**3. Train Embedding Layer**\n",
        "\n",
        "In this section, we will learn a word embedding while training a neural network on the classification problem.\n",
        "\n",
        "A word embedding is a way of representing text where each word in the vocabulary is represented by a real valued vector in a high-dimensional space. The vectors are learned in such a way that words that have similar meanings will have similar representation in the vector space (close in the vector space). This is a more expressive representation for text than more classical methods like bag-of-words, where relationships between words or tokens are ignored, or forced in bigram and trigram approaches.\n",
        "\n",
        "The real valued vector representation for words can be learned while training the neural network. We can do this in the Keras deep learning library using the Embedding layer.\n",
        "\n",
        "If you are new to word embeddings, see the post:\n",
        "\n",
        "[What Are Word Embeddings for Text?](https://machinelearningmastery.com/what-are-word-embeddings/)\n",
        "\n",
        "If you are new to word embedding layers in Keras, see the post:\n",
        "\n",
        "[How to Use Word Embedding Layers for Deep Learning with Keras](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)\n",
        "\n",
        "The first step is to load the vocabulary. We will use it to filter out words from movie reviews that we are not interested in.\n",
        "\n",
        "If you have worked through the previous section, you should have a local file called ‘vocab.txt‘ with one word per line. We can load that file and build a vocabulary as a set for checking the validity of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whsF4qhlbTKm"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLqOli-MbV69"
      },
      "source": [
        "Next, we need to load all of the training data movie reviews. For that we can adapt the process_docs() from the previous section to load the documents, clean them, and return them as a list of strings, with one document per string. We want each document to be a string for easy encoding as a sequence of integers later.\n",
        "\n",
        "Cleaning the document involves splitting each review based on white space, removing punctuation, and then filtering out all tokens not in the vocabulary.\n",
        "\n",
        "The updated clean_doc() function is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV2tj31FbXZ-"
      },
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# filter out tokens not in vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\ttokens = ' '.join(tokens)\n",
        "\treturn tokens"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0VKgO8BeH6s"
      },
      "source": [
        "The updated process_docs() can then call the clean_doc() for each document on the ‘pos‘ and ‘neg‘ directories that are in our training dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOa6pVeTeIeM"
      },
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "\tdocuments = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_trian and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load the doc\n",
        "\t\tdoc = load_doc(path)\n",
        "\t\t# clean doc\n",
        "\t\ttokens = clean_doc(doc, vocab)\n",
        "\t\t# add to list\n",
        "\t\tdocuments.append(tokens)\n",
        "\treturn documents\n",
        "\n",
        "# load all training reviews\n",
        "positive_docs = process_docs(data_path + 'txt_sentoken/pos', vocab, True)\n",
        "negative_docs = process_docs(data_path + 'txt_sentoken/neg', vocab, True)\n",
        "train_docs = negative_docs + positive_docs"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJxk574ZbcGV"
      },
      "source": [
        "The next step is to encode each document as a sequence of integers.\n",
        "\n",
        "The Keras Embedding layer requires integer inputs where each integer maps to a single token that has a specific real-valued vector representation within the embedding. These vectors are random at the beginning of training, but during training become meaningful to the network.\n",
        "\n",
        "We can encode the training documents as sequences of integers using the Tokenizer class in the Keras API.\n",
        "\n",
        "First, we must construct an instance of the class then train it on all documents in the training dataset. In this case, it develops a vocabulary of all tokens in the training dataset and develops a consistent mapping from words in the vocabulary to unique integers. We could just as easily develop this mapping ourselves using our vocabulary file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ribKtsui-aq",
        "outputId": "86802060-7c11-4d92-ae64-33dcbd630c52"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf-C8sYybdfV"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "#from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(train_docs)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jagb1RY4bfqd"
      },
      "source": [
        "Now that the mapping of words to integers has been prepared, we can use it to encode the reviews in the training dataset. We can do that by calling the texts_to_sequences() function on the Tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfN5YNs8bjmd"
      },
      "source": [
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpGYuguSbqHe"
      },
      "source": [
        "We also need to ensure that all documents have the same length.\n",
        "\n",
        "This is a requirement of Keras for efficient computation. We could truncate reviews to the smallest size or zero-pad (pad with the value ‘0’) reviews to the maximum length, or some hybrid. In this case, we will pad all reviews to the length of the longest review in the training dataset.\n",
        "\n",
        "First, we can find the longest review using the max() function on the training dataset and take its length. We can then call the Keras function pad_sequences() to pad the sequences to the maximum length by adding 0 values on the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgHD6w5lbrKF"
      },
      "source": [
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "#from keras_preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7bFnwBabsL_"
      },
      "source": [
        "Finally, we can define the class labels for the training dataset, needed to fit the supervised neural network model to predict the sentiment of reviews.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj0jj45ybtRm"
      },
      "source": [
        "from numpy import array\n",
        "\n",
        "# define training labels\n",
        "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXUnNIUKbwGO"
      },
      "source": [
        "We can then encode and pad the test dataset, needed later to evaluate the model after we train it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2aF-wXGbxPG"
      },
      "source": [
        "# load all test reviews\n",
        "positive_docs = process_docs(data_path + 'txt_sentoken/pos', vocab, False)\n",
        "negative_docs = process_docs(data_path + 'txt_sentoken/neg', vocab, False)\n",
        "test_docs = negative_docs + positive_docs\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "# pad sequences\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define test labels\n",
        "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE-9UYd3byQW"
      },
      "source": [
        "We are now ready to define our neural network model.\n",
        "\n",
        "The model will use an Embedding layer as the first hidden layer. The Embedding requires the specification of the vocabulary size, the size of the real-valued vector space, and the maximum length of input documents.\n",
        "\n",
        "The vocabulary size is the total number of words in our vocabulary, plus one for unknown words. This could be the vocab set length or the size of the vocab within the tokenizer used to integer encode the documents, for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Em022EbzI3"
      },
      "source": [
        "# define vocabulary size (largest integer value)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV1Euiw8b06O"
      },
      "source": [
        "We will use a 100-dimensional vector space, but you could try other values, such as 50 or 150. Finally, the maximum document length was calculated above in the max_length variable used during padding.\n",
        "\n",
        "The complete model definition is listed below including the Embedding layer.\n",
        "\n",
        "We use a Convolutional Neural Network (CNN) as they have proven to be successful at document classification problems. A conservative CNN configuration is used with 32 filters (parallel fields for processing words) and a kernel size of 8 with a rectified linear (‘relu’) activation function. This is followed by a pooling layer that reduces the output of the convolutional layer by half.\n",
        "\n",
        "Next, the 2D output from the CNN part of the model is flattened to one long 2D vector to represent the ‘features’ extracted by the CNN. The back-end of the model is a standard Multilayer Perceptron layers to interpret the CNN features. The output layer uses a sigmoid activation function to output a value between 0 and 1 for the negative and positive sentiment in the review.\n",
        "\n",
        "For more advice on effective deep learning model configuration for text classification, see the post:\n",
        "\n",
        "Best Practices for Document Classification with Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "\n",
        "# First, ensure these variables are properly defined\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "print(f\"Maximum Length: {max_length}\")\n",
        "\n",
        "# Define model with explicit input shape\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 100, input_length=max_length),\n",
        "    Conv1D(filters=32, kernel_size=8, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(10, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Build the model with sample input shape\n",
        "model.build(input_shape=(None, max_length))\n",
        "\n",
        "# Now print the summary\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "a4NNiuS9lDiu",
        "outputId": "4f711c37-58d8-4e63-e812-9efc86f9844d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 25768\n",
            "Maximum Length: 1317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1317\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │       \u001b[38;5;34m2,576,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1310\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │          \u001b[38;5;34m25,632\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m655\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20960\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │         \u001b[38;5;34m209,610\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m11\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1317</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,576,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1310</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">25,632</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">655</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20960</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">209,610</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,812,053\u001b[0m (10.73 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,812,053</span> (10.73 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,812,053\u001b[0m (10.73 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,812,053</span> (10.73 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGOqLkwPb29W"
      },
      "source": [
        "Running just this piece provides a summary of the defined network.\n",
        "\n",
        "We can see that the Embedding layer expects documents with a length of 442 words as input and encodes each word in the document as a 100 element vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwoCSHKKb60t"
      },
      "source": [
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #\n",
        "=================================================================\n",
        "embedding_1 (Embedding)      (None, 442, 100)          2576800\n",
        "_________________________________________________________________\n",
        "conv1d_1 (Conv1D)            (None, 435, 32)           25632\n",
        "_________________________________________________________________\n",
        "max_pooling1d_1 (MaxPooling1 (None, 217, 32)           0\n",
        "_________________________________________________________________\n",
        "flatten_1 (Flatten)          (None, 6944)              0\n",
        "_________________________________________________________________\n",
        "dense_1 (Dense)              (None, 10)                69450\n",
        "_________________________________________________________________\n",
        "dense_2 (Dense)              (None, 1)                 11\n",
        "=================================================================\n",
        "Total params: 2,671,893\n",
        "Trainable params: 2,671,893\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrk9jij7b8ZN"
      },
      "source": [
        "Next, we fit the network on the training data.\n",
        "\n",
        "We use a binary cross entropy loss function because the problem we are learning is a binary classification problem. The efficient Adam implementation of stochastic gradient descent is used and we keep track of accuracy in addition to loss during training. The model is trained for 10 epochs, or 10 passes through the training data.\n",
        "\n",
        "The network configuration and training schedule were found with a little trial and error, but are by no means optimal for this problem. If you can get better results with a different configuration, let me know."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUKA2a6lb9Jd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c398a2-4672-4b35-9e25-a0bed42ce700"
      },
      "source": [
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "57/57 - 15s - 263ms/step - accuracy: 0.5289 - loss: 0.6892\n",
            "Epoch 2/10\n",
            "57/57 - 20s - 353ms/step - accuracy: 0.6622 - loss: 0.6142\n",
            "Epoch 3/10\n",
            "57/57 - 21s - 370ms/step - accuracy: 0.8678 - loss: 0.4456\n",
            "Epoch 4/10\n",
            "57/57 - 15s - 256ms/step - accuracy: 0.9594 - loss: 0.3509\n",
            "Epoch 5/10\n",
            "57/57 - 19s - 335ms/step - accuracy: 0.9817 - loss: 0.3120\n",
            "Epoch 6/10\n",
            "57/57 - 14s - 238ms/step - accuracy: 0.9911 - loss: 0.2864\n",
            "Epoch 7/10\n",
            "57/57 - 14s - 240ms/step - accuracy: 0.9944 - loss: 0.2703\n",
            "Epoch 8/10\n",
            "57/57 - 14s - 242ms/step - accuracy: 0.9956 - loss: 0.2568\n",
            "Epoch 9/10\n",
            "57/57 - 14s - 240ms/step - accuracy: 0.9961 - loss: 0.2449\n",
            "Epoch 10/10\n",
            "57/57 - 14s - 243ms/step - accuracy: 0.9961 - loss: 0.2343\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78f516cbff50>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVXqlpP-b-gd"
      },
      "source": [
        "After the model is fit, it is evaluated on the test dataset. This dataset contains words that we have not seen before and reviews not seen during training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UJpgfNrb_LU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05dafa12-9784-4d35-e1fe-e3a6c6e74c8d"
      },
      "source": [
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 81.999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nACeOeJscAT8"
      },
      "source": [
        "We can tie all of this together.\n",
        "\n",
        "The complete code listing is provided below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-Ll5wvseWAI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "outputId": "e9e378fa-068a-4279-f411-897b33bbcc08"
      },
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "#from keras.layers.convolutional import Conv1D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "\n",
        "#from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# filter out tokens not in vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\ttokens = ' '.join(tokens)\n",
        "\treturn tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "\tdocuments = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_trian and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load the doc\n",
        "\t\tdoc = load_doc(path)\n",
        "\t\t# clean doc\n",
        "\t\ttokens = clean_doc(doc, vocab)\n",
        "\t\t# add to list\n",
        "\t\tdocuments.append(tokens)\n",
        "\treturn documents\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "# load all training reviews\n",
        "positive_docs = process_docs(data_path + 'txt_sentoken/pos', vocab, True)\n",
        "negative_docs = process_docs(data_path + 'txt_sentoken/neg', vocab, True)\n",
        "train_docs = negative_docs + positive_docs\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define training labels\n",
        "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
        "\n",
        "# load all test reviews\n",
        "positive_docs = process_docs(data_path + 'txt_sentoken/pos', vocab, False)\n",
        "negative_docs = process_docs(data_path + 'txt_sentoken/neg', vocab, False)\n",
        "test_docs = negative_docs + positive_docs\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "# pad sequences\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define test labels\n",
        "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
        "\n",
        "# define vocabulary size (largest integer value)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "# First, ensure these variables are properly defined\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "print(f\"Maximum Length: {max_length}\")\n",
        "\n",
        "# Define model with explicit input shape\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 100, input_length=max_length),\n",
        "    Conv1D(filters=32, kernel_size=8, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(10, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Build the model with sample input shape\n",
        "model.build(input_shape=(None, max_length))\n",
        "\n",
        "# Now print the summary\n",
        "print(model.summary())\n",
        "\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "\n",
        "\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 25768\n",
            "Maximum Length: 1317\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1317\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │       \u001b[38;5;34m2,576,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1310\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │          \u001b[38;5;34m25,632\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m655\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20960\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │         \u001b[38;5;34m209,610\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m11\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1317</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,576,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1310</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">25,632</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">655</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20960</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">209,610</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,812,053\u001b[0m (10.73 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,812,053</span> (10.73 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,812,053\u001b[0m (10.73 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,812,053</span> (10.73 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/10\n",
            "57/57 - 15s - 265ms/step - accuracy: 0.5267 - loss: 0.6882\n",
            "Epoch 2/10\n",
            "57/57 - 13s - 228ms/step - accuracy: 0.7811 - loss: 0.5393\n",
            "Epoch 3/10\n",
            "57/57 - 13s - 224ms/step - accuracy: 0.9717 - loss: 0.1080\n",
            "Epoch 4/10\n",
            "57/57 - 20s - 347ms/step - accuracy: 0.9994 - loss: 0.0090\n",
            "Epoch 5/10\n",
            "57/57 - 22s - 378ms/step - accuracy: 1.0000 - loss: 0.0026\n",
            "Epoch 6/10\n",
            "57/57 - 20s - 354ms/step - accuracy: 1.0000 - loss: 0.0015\n",
            "Epoch 7/10\n",
            "57/57 - 21s - 362ms/step - accuracy: 1.0000 - loss: 0.0011\n",
            "Epoch 8/10\n",
            "57/57 - 20s - 353ms/step - accuracy: 1.0000 - loss: 8.7714e-04\n",
            "Epoch 9/10\n",
            "57/57 - 12s - 213ms/step - accuracy: 1.0000 - loss: 7.3304e-04\n",
            "Epoch 10/10\n",
            "57/57 - 21s - 374ms/step - accuracy: 1.0000 - loss: 6.2881e-04\n",
            "Test Accuracy: 85.500002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIWa7mNV-r9j"
      },
      "source": [
        "Running the example prints the loss and accuracy at the end of each training epoch.\n",
        "\n",
        "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
        "\n",
        "We can see that the model very quickly achieves 100% accuracy on the training dataset. At the end of the run, the model achieves an accuracy of 84.5% on the test dataset, which is a great score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi5hRIqM-wJL"
      },
      "source": [
        "We can see that the model very quickly achieves 100% accuracy on the training dataset. At the end of the run, the model achieves an accuracy of 84.5% on the test dataset, which is a great score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejvNrGYv-xZz"
      },
      "source": [
        "We have just seen an example of how we can learn a word embedding as part of fitting a neural network model.\n",
        "\n",
        "Next, let’s look at how we can efficiently learn a standalone embedding that we could later use in our neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krFZV8jv-0yU"
      },
      "source": [
        "# **4. Train word2vec Embedding**\n",
        "\n",
        "In this section, we will discover how to learn a standalone word embedding using an efficient algorithm called word2vec.\n",
        "\n",
        "A downside of learning a word embedding as part of the network is that it can be very slow, especially for very large text datasets.\n",
        "\n",
        "The word2vec algorithm is an approach to learning a word embedding from a text corpus in a standalone way. The benefit of the method is that it can produce high-quality word embeddings very efficiently, in terms of space and time complexity.\n",
        "\n",
        "The first step is to prepare the documents ready for learning the embedding.\n",
        "\n",
        "This involves the same data cleaning steps from the previous section, namely splitting documents by their white space, removing punctuation, and filtering out tokens not in the vocabulary.\n",
        "\n",
        "The word2vec algorithm processes documents sentence by sentence. This means we will preserve the sentence-based structure during cleaning.\n",
        "\n",
        "We start by loading the vocabulary, as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJJDRSGT-_je"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYOCP1fK_BN3"
      },
      "source": [
        "Next, we define a function named doc_to_clean_lines() to clean a loaded document line by line and return a list of the cleaned lines.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGJTixLy_Dz3"
      },
      "source": [
        "# turn a doc into clean tokens\n",
        "def doc_to_clean_lines(doc, vocab):\n",
        "\tclean_lines = list()\n",
        "\tlines = doc.splitlines()\n",
        "\tfor line in lines:\n",
        "\t\t# split into tokens by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# remove punctuation from each token\n",
        "\t\ttable = str.maketrans('', '', punctuation)\n",
        "\t\ttokens = [w.translate(table) for w in tokens]\n",
        "\t\t# filter out tokens not in vocab\n",
        "\t\ttokens = [w for w in tokens if w in vocab]\n",
        "\t\tclean_lines.append(tokens)\n",
        "\treturn clean_lines"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgKo933m_EoO"
      },
      "source": [
        "Next, we adapt the process_docs() function to load and clean all of the documents in a folder and return a list of all document lines.\n",
        "\n",
        "The results from this function will be the training data for the word2vec model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IabhIiLo_JwH"
      },
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_trian and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tdoc = load_doc(path)\n",
        "\t\tdoc_lines = doc_to_clean_lines(doc, vocab)\n",
        "\t\t# add lines to list\n",
        "\t\tlines += doc_lines\n",
        "\treturn lines"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_OQ5pDg_K9h"
      },
      "source": [
        "We can then load all of the training data and convert it into a long list of ‘sentences’ (lists of tokens) ready for fitting the word2vec model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcmkr5bi_L-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0168a6d1-9f26-4ead-d880-aaeff499a7b9"
      },
      "source": [
        "# load training data\n",
        "positive_docs = process_docs(data_path + 'txt_sentoken/pos', vocab, True)\n",
        "negative_docs = process_docs(data_path + 'txt_sentoken/neg', vocab, True)\n",
        "sentences = negative_docs + positive_docs\n",
        "print('Total training sentences: %d' % len(sentences))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training sentences: 58109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AbW5yFd_ehN"
      },
      "source": [
        "We will use the word2vec implementation provided in the Gensim Python library. Specifically the Word2Vec class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEqELm4U_hcl"
      },
      "source": [
        "The model is fit when constructing the class. We pass in the list of clean sentences from the training data, then specify the size of the embedding vector space (we use 100 again), the number of neighboring words to look at when learning how to embed each word in the training sentences (we use 5 neighbors), the number of threads to use when fitting the model (we use 8, but change this if you have more or less CPU cores), and the minimum occurrence count for words to consider in the vocabulary (we set this to 1 as we have already prepared the vocabulary).\n",
        "\n",
        "After the model is fit, we print the size of the learned vocabulary, which should match the size of our vocabulary in vocab.txt of 25,767 tokens."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Assuming 'sentences' is already defined and preprocessed\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, workers=8, min_count=1)\n",
        "\n",
        "# Summarize vocabulary size in model using the updated API\n",
        "words = list(model.wv.index_to_key)  # List of words in the model\n",
        "print('Vocabulary size: %d' % len(words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imCjVoWineMI",
        "outputId": "8311c7a9-c093-40dd-d702-b5eecf41a303"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 25767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjfEqrSN_jL8"
      },
      "source": [
        "Finally, we save the learned embedding vectors to file using the save_word2vec_format() on the model’s ‘wv‘ (word vector) attribute. The embedding is saved in ASCII format with one word and vector per line.\n",
        "\n",
        "The complete example is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN26-qPt_j_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04995ad3-9f33-432d-dbfa-f2b74e2fb632"
      },
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def doc_to_clean_lines(doc, vocab):\n",
        "\tclean_lines = list()\n",
        "\tlines = doc.splitlines()\n",
        "\tfor line in lines:\n",
        "\t\t# split into tokens by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# remove punctuation from each token\n",
        "\t\ttable = str.maketrans('', '', punctuation)\n",
        "\t\ttokens = [w.translate(table) for w in tokens]\n",
        "\t\t# filter out tokens not in vocab\n",
        "\t\ttokens = [w for w in tokens if w in vocab]\n",
        "\t\tclean_lines.append(tokens)\n",
        "\treturn clean_lines\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_trian and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tdoc = load_doc(path)\n",
        "\t\tdoc_lines = doc_to_clean_lines(doc, vocab)\n",
        "\t\t# add lines to list\n",
        "\t\tlines += doc_lines\n",
        "\treturn lines\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "# load training data\n",
        "positive_docs = process_docs(data_path + 'txt_sentoken/pos', vocab, True)\n",
        "negative_docs = process_docs(data_path + 'txt_sentoken/neg', vocab, True)\n",
        "sentences = negative_docs + positive_docs\n",
        "print('Total training sentences: %d' % len(sentences))\n",
        "\n",
        "# train word2vec model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, workers=8, min_count=1)\n",
        "\n",
        "# Summarize vocabulary size in the model using the correct attribute\n",
        "words = list(model.wv.index_to_key)  # Use .index_to_key instead of .vocab\n",
        "print('Vocabulary size: %d' % len(words))\n",
        "\n",
        "# save model in ASCII (word2vec) format\n",
        "filename = 'embedding_word2vec.txt'\n",
        "model.wv.save_word2vec_format(filename, binary=False)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training sentences: 58109\n",
            "Vocabulary size: 25767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qD-HZRf_lSt"
      },
      "source": [
        "Running the example loads 58,109 sentences from the training data and creates an embedding for a vocabulary of 25,767 words.\n",
        "\n",
        "You should now have a file ’embedding_word2vec.txt’ with the learned vectors in your current working directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ona8eOQ_nck"
      },
      "source": [
        "```Total training sentences: 58109\n",
        "Vocabulary size: 25767\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsk9kwpa_vHY"
      },
      "source": [
        "Next, let’s look at using these learned vectors in our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_4dDgDm_wvg"
      },
      "source": [
        "# **5. Use Pre-trained Embedding**\n",
        "\n",
        "In this section, we will use a pre-trained word embedding prepared on a very large text corpus.\n",
        "\n",
        "We can use the pre-trained word embedding developed in the previous section and the CNN model developed in the section before that.\n",
        "\n",
        "The first step is to load the word embedding as a directory of words to vectors. The word embedding was saved in so-called ‘word2vec‘ format that contains a header line. We will skip this header line when loading the embedding.\n",
        "\n",
        "The function below named load_embedding() loads the embedding and returns a directory of words mapped to the vectors in NumPy format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY6BrsbS_zQn"
      },
      "source": [
        "# load embedding as a dict\n",
        "def load_embedding(filename):\n",
        "\t# load embedding into memory, skip first line\n",
        "\tfile = open(filename,'r')\n",
        "\tlines = file.readlines()[1:]\n",
        "\tfile.close()\n",
        "\t# create a map of words to vectors\n",
        "\tembedding = dict()\n",
        "\tfor line in lines:\n",
        "\t\tparts = line.split()\n",
        "\t\t# key is string word, value is numpy array for vector\n",
        "\t\tembedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
        "\treturn embedding"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGgFaKp6_0n3"
      },
      "source": [
        "Now that we have all of the vectors in memory, we can order them in such a way as to match the integer encoding prepared by the Keras Tokenizer.\n",
        "\n",
        "Recall that we integer encode the review documents prior to passing them to the Embedding layer. The integer maps to the index of a specific vector in the embedding layer. Therefore, it is important that we lay the vectors out in the Embedding layer such that the encoded words map to the correct vector.\n",
        "\n",
        "Below defines a function get_weight_matrix() that takes the loaded embedding and the tokenizer.word_index vocabulary as arguments and returns a matrix with the word vectors in the correct locations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b9axODD_1qQ"
      },
      "source": [
        "# create a weight matrix for the Embedding layer from a loaded embedding\n",
        "def get_weight_matrix(embedding, vocab):\n",
        "\t# total vocabulary size plus 0 for unknown words\n",
        "\tvocab_size = len(vocab) + 1\n",
        "\t# define weight matrix dimensions with all 0\n",
        "\tweight_matrix = zeros((vocab_size, 100))\n",
        "\t# step vocab, store vectors using the Tokenizer's integer mapping\n",
        "\tfor word, i in vocab.items():\n",
        "\t\tweight_matrix[i] = embedding.get(word)\n",
        "\treturn weight_matrix"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-281crYr_2ih"
      },
      "source": [
        "Now we can use these functions to create our new Embedding layer for our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK1b02zF_3fa"
      },
      "source": [
        "...\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "# load embedding from file\n",
        "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
        "# get vectors in the right order\n",
        "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
        "# create the embedding layer\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8LYe8N6_41Z"
      },
      "source": [
        "Note that the prepared weight matrix embedding_vectors is passed to the new Embedding layer as an argument and that we set the ‘trainable‘ argument to ‘False‘ to ensure that the network does not try to adapt the pre-learned vectors as part of training the network.\n",
        "\n",
        "We can now add this layer to our model. We also have a slightly different model configuration with a lot more filters (128) in the CNN model and a kernel that matches the 5 words used as neighbors when developing the word2vec embedding. Finally, the back-end of the model was simplified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnNrZUhv_5sq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "7943f2e7-68e7-483c-ac45-b834dc0b8300"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'embedding_layer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-01513bd4a398>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# define model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'embedding_layer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw8ZOxDe_63q"
      },
      "source": [
        "These changes were found with a little trial and error.\n",
        "\n",
        "The complete code listing is provided below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "#import tensorflow as tf\n",
        "#print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-oV8tl0o0IX",
        "outputId": "eef343c5-1106-4e7b-e001-8c1893325e5f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from numpy import array, asarray, zeros\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding, Conv1D, MaxPooling1D\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # filter out tokens not in vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    tokens = ' '.join(tokens)\n",
        "    return tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "    documents = list()\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "        # skip any reviews in the test set\n",
        "        if is_trian and filename.startswith('cv9'):\n",
        "            continue\n",
        "        if not is_trian and not filename.startswith('cv9'):\n",
        "            continue\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # load the doc\n",
        "        doc = load_doc(path)\n",
        "        # clean doc\n",
        "        tokens = clean_doc(doc, vocab)\n",
        "        # add to list\n",
        "        documents.append(tokens)\n",
        "    return documents\n",
        "\n",
        "# load embedding as a dict\n",
        "def load_embedding(filename):\n",
        "    # load embedding into memory, skip first line\n",
        "    file = open(filename,'r')\n",
        "    lines = file.readlines()[1:]\n",
        "    file.close()\n",
        "    # create a map of words to vectors\n",
        "    embedding = dict()\n",
        "    for line in lines:\n",
        "        parts = line.split()\n",
        "        # key is string word, value is numpy array for vector\n",
        "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
        "    return embedding\n",
        "\n",
        "# create a weight matrix for the Embedding layer from a loaded embedding\n",
        "def get_weight_matrix(embedding, vocab):\n",
        "    # total vocabulary size plus 0 for unknown words\n",
        "    vocab_size = len(vocab) + 1\n",
        "    # define weight matrix dimensions with all 0\n",
        "    weight_matrix = zeros((vocab_size, 100))\n",
        "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "    for word, i in vocab.items():\n",
        "        weight_matrix[i] = embedding.get(word)\n",
        "    return weight_matrix\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "# load all training reviews\n",
        "positive_docs = process_docs(data_path + 'txt_sentoken/pos', vocab, True)\n",
        "negative_docs = process_docs(data_path + 'txt_sentoken/neg', vocab, True)\n",
        "train_docs = negative_docs + positive_docs\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define training labels\n",
        "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
        "\n",
        "# load all test reviews\n",
        "positive_docs = process_docs(data_path + 'txt_sentoken/pos', vocab, False)\n",
        "negative_docs = process_docs(data_path + 'txt_sentoken/neg', vocab, False)\n",
        "test_docs = negative_docs + positive_docs\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "# pad sequences\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define test labels\n",
        "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
        "\n",
        "# define vocabulary size (largest integer value)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# load embedding from file\n",
        "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
        "# get vectors in the right order\n",
        "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, weights=[embedding_vectors],\n",
        "                   input_length=max_length, trainable=False))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu',\n",
        "                 input_shape=(max_length, 100)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Build model explicitly\n",
        "model.build(input_shape=(None, max_length))\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit network with validation split\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2, validation_split=0.2)\n",
        "\n",
        "# Evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "id": "eYi9RV3Oqrm_",
        "outputId": "183f2878-cc56-4cf2-ffc5-47cbaa6314d1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1317\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │       \u001b[38;5;34m2,576,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1313\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │          \u001b[38;5;34m64,128\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m656\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83968\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │          \u001b[38;5;34m83,969\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1317</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,576,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">64,128</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83968</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">83,969</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,724,897\u001b[0m (10.39 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,724,897</span> (10.39 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m148,097\u001b[0m (578.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">148,097</span> (578.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,576,800\u001b[0m (9.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,576,800</span> (9.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/10\n",
            "45/45 - 15s - 344ms/step - accuracy: 0.5924 - loss: 0.7138 - val_accuracy: 0.1444 - val_loss: 0.8044\n",
            "Epoch 2/10\n",
            "45/45 - 20s - 447ms/step - accuracy: 0.6292 - loss: 0.6573 - val_accuracy: 0.0167 - val_loss: 1.0297\n",
            "Epoch 3/10\n",
            "45/45 - 20s - 452ms/step - accuracy: 0.6472 - loss: 0.6294 - val_accuracy: 0.4083 - val_loss: 0.7149\n",
            "Epoch 4/10\n",
            "45/45 - 21s - 460ms/step - accuracy: 0.7028 - loss: 0.5681 - val_accuracy: 0.2556 - val_loss: 0.8750\n",
            "Epoch 5/10\n",
            "45/45 - 20s - 439ms/step - accuracy: 0.7889 - loss: 0.4681 - val_accuracy: 0.3056 - val_loss: 1.0449\n",
            "Epoch 6/10\n",
            "45/45 - 21s - 472ms/step - accuracy: 0.8618 - loss: 0.3617 - val_accuracy: 0.1583 - val_loss: 1.6290\n",
            "Epoch 7/10\n",
            "45/45 - 20s - 450ms/step - accuracy: 0.9215 - loss: 0.2713 - val_accuracy: 0.1667 - val_loss: 2.0627\n",
            "Epoch 8/10\n",
            "45/45 - 21s - 459ms/step - accuracy: 0.9556 - loss: 0.1834 - val_accuracy: 0.4444 - val_loss: 1.3371\n",
            "Epoch 9/10\n",
            "45/45 - 21s - 469ms/step - accuracy: 0.9819 - loss: 0.1204 - val_accuracy: 0.2472 - val_loss: 2.3491\n",
            "Epoch 10/10\n",
            "45/45 - 14s - 303ms/step - accuracy: 0.9931 - loss: 0.0773 - val_accuracy: 0.4222 - val_loss: 1.7208\n",
            "Test Accuracy: 56.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9KGlHBCACFa"
      },
      "source": [
        "Running the example shows that performance was not improved.\n",
        "\n",
        "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
        "\n",
        "In fact, performance was a lot worse. The results show that the training dataset was learned successfully, but evaluation on the test dataset was very poor, at just above 50% accuracy.\n",
        "\n",
        "The cause of the poor test performance may be because of the chosen word2vec configuration or the chosen neural network configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLEXDTsaADS5"
      },
      "source": [
        "```...\n",
        "Epoch 6/10\n",
        "2s - loss: 0.3306 - acc: 0.8778\n",
        "Epoch 7/10\n",
        "2s - loss: 0.2888 - acc: 0.8917\n",
        "Epoch 8/10\n",
        "2s - loss: 0.1878 - acc: 0.9439\n",
        "Epoch 9/10\n",
        "2s - loss: 0.1255 - acc: 0.9750\n",
        "Epoch 10/10\n",
        "2s - loss: 0.0812 - acc: 0.9928\n",
        "Test Accuracy: 53.000000\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWqt70muAImM"
      },
      "source": [
        "The weights in the embedding layer can be used as a starting point for the network, and adapted during the training of the network. We can do this by setting ‘trainable=True‘ (the default) in the creation of the embedding layer.\n",
        "\n",
        "Repeating the experiment with this change shows slightly better results, but still poor.\n",
        "\n",
        "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
        "\n",
        "I would encourage you to explore alternate configurations of the embedding and network to see if you can do better. Let me know how you do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gm2SLTZAJdD"
      },
      "source": [
        "```\n",
        "...\n",
        "Epoch 6/10\n",
        "4s - loss: 0.0950 - acc: 0.9917\n",
        "Epoch 7/10\n",
        "4s - loss: 0.0355 - acc: 0.9983\n",
        "Epoch 8/10\n",
        "4s - loss: 0.0158 - acc: 1.0000\n",
        "Epoch 9/10\n",
        "4s - loss: 0.0080 - acc: 1.0000\n",
        "Epoch 10/10\n",
        "4s - loss: 0.0050 - acc: 1.0000\n",
        "Test Accuracy: 57.500000\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjeq-UnEAU2V"
      },
      "source": [
        "It is possible to use pre-trained word vectors prepared on very large corpora of text data.\n",
        "\n",
        "For example, both Google and Stanford provide pre-trained word vectors that you can download, trained with the efficient word2vec and GloVe methods respectively.\n",
        "\n",
        "Let’s try to use pre-trained vectors in our model.\n",
        "\n",
        "You can download [pre-trained GloVe vectors](https://nlp.stanford.edu/projects/glove/) from the Stanford webpage. Specifically, vectors trained on Wikipedia data:\n",
        "\n",
        "- [glove.6B.zip (822 Megabyte download)](http://nlp.stanford.edu/data/glove.6B.zip)\n",
        "\n",
        "Unzipping the file, you will find pre-trained embeddings for various different dimensions. We will load the 100 dimension version in the file ‘glove.6B.100d.txt‘\n",
        "\n",
        "The Glove file does not contain a header file, so we do not need to skip the first line when loading the embedding into memory. The updated load_embedding() function is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQtTwDHhAd3l"
      },
      "source": [
        "# load embedding as a dict\n",
        "def load_embedding(filename):\n",
        "\t# load embedding into memory, skip first line\n",
        "\tfile = open(filename,'r')\n",
        "\tlines = file.readlines()\n",
        "\tfile.close()\n",
        "\t# create a map of words to vectors\n",
        "\tembedding = dict()\n",
        "\tfor line in lines:\n",
        "\t\tparts = line.split()\n",
        "\t\t# key is string word, value is numpy array for vector\n",
        "\t\tembedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
        "\treturn embedding"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdKextWuAfVM"
      },
      "source": [
        "It is possible that the loaded embedding does not contain all of the words in our chosen vocabulary. As such, when creating the Embedding weight matrix, we need to skip words that do not have a corresponding vector in the loaded GloVe data. Below is the updated, more defensive version of the get_weight_matrix() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K91_UfSuAgT0"
      },
      "source": [
        "# create a weight matrix for the Embedding layer from a loaded embedding\n",
        "def get_weight_matrix(embedding, vocab):\n",
        "\t# total vocabulary size plus 0 for unknown words\n",
        "\tvocab_size = len(vocab) + 1\n",
        "\t# define weight matrix dimensions with all 0\n",
        "\tweight_matrix = zeros((vocab_size, 100))\n",
        "\t# step vocab, store vectors using the Tokenizer's integer mapping\n",
        "\tfor word, i in vocab.items():\n",
        "\t\tvector = embedding.get(word)\n",
        "\t\tif vector is not None:\n",
        "\t\t\tweight_matrix[i] = vector\n",
        "\treturn weight_matrix"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYNqx3E4AhT0"
      },
      "source": [
        "We can now load the GloVe embedding and create the Embedding layer as before.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySHvvnwBAiH7"
      },
      "source": [
        "# load embedding from file\n",
        "raw_embedding = load_embedding(data_path2 + 'glove.6B.100d.txt')\n",
        "# get vectors in the right order\n",
        "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
        "# create the embedding layer\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL7CTYuXAjDU"
      },
      "source": [
        "We will use the same model as before.\n",
        "\n",
        "The complete example is listed below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding, Conv1D, MaxPooling1D\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # filter out tokens not in vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    tokens = ' '.join(tokens)\n",
        "    return tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "    documents = list()\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "        # skip any reviews in the test set\n",
        "        if is_trian and filename.startswith('cv9'):\n",
        "            continue\n",
        "        if not is_trian and not filename.startswith('cv9'):\n",
        "            continue\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # load the doc\n",
        "        doc = load_doc(path)\n",
        "        # clean doc\n",
        "        tokens = clean_doc(doc, vocab)\n",
        "        # add to list\n",
        "        documents.append(tokens)\n",
        "    return documents\n",
        "\n",
        "# load embedding as a dict\n",
        "def load_embedding(filename):\n",
        "    # load embedding into memory, skip first line\n",
        "    file = open(filename,'r')\n",
        "    lines = file.readlines()\n",
        "    file.close()\n",
        "    # create a map of words to vectors\n",
        "    embedding = dict()\n",
        "    for line in lines:\n",
        "        parts = line.split()\n",
        "        # key is string word, value is numpy array for vector\n",
        "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
        "    return embedding\n",
        "\n",
        "# create a weight matrix for the Embedding layer from a loaded embedding\n",
        "def get_weight_matrix(embedding, vocab):\n",
        "    # total vocabulary size plus 0 for unknown words\n",
        "    vocab_size = len(vocab) + 1\n",
        "    # define weight matrix dimensions with all 0\n",
        "    weight_matrix = zeros((vocab_size, 100))\n",
        "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "    for word, i in vocab.items():\n",
        "        vector = embedding.get(word)\n",
        "        if vector is not None:\n",
        "            weight_matrix[i] = vector\n",
        "    return weight_matrix\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "# load all training reviews\n",
        "positive_docs = process_docs(data_path + 'txt_sentoken/pos', vocab, True)\n",
        "negative_docs = process_docs(data_path + 'txt_sentoken/neg', vocab, True)\n",
        "train_docs = negative_docs + positive_docs\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define training labels\n",
        "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
        "\n",
        "# load all test reviews\n",
        "positive_docs = process_docs(data_path + 'txt_sentoken/pos', vocab, False)\n",
        "negative_docs = process_docs(data_path + 'txt_sentoken/neg', vocab, False)\n",
        "test_docs = negative_docs + positive_docs\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "# pad sequences\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define test labels\n",
        "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
        "\n",
        "# define vocabulary size (largest integer value)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# load embedding from file\n",
        "raw_embedding = load_embedding(data_path2 + 'glove.6B.100d.txt')\n",
        "# get vectors in the right order\n",
        "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, weights=[embedding_vectors],\n",
        "                   input_length=max_length, trainable=False))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu',\n",
        "                 input_shape=(max_length, 100)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Build model explicitly\n",
        "model.build(input_shape=(None, max_length))\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2, validation_split=0.2)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "V2Dzm_slsBLy",
        "outputId": "441dd6e8-7596-4d8f-de62-1468c1709f82"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1317\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │       \u001b[38;5;34m2,576,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1313\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │          \u001b[38;5;34m64,128\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m656\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83968\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │          \u001b[38;5;34m83,969\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1317</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,576,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">64,128</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83968</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">83,969</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,724,897\u001b[0m (10.39 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,724,897</span> (10.39 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m148,097\u001b[0m (578.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">148,097</span> (578.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,576,800\u001b[0m (9.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,576,800</span> (9.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/10\n",
            "45/45 - 15s - 330ms/step - accuracy: 0.6132 - loss: 0.6988 - val_accuracy: 0.3222 - val_loss: 0.7560\n",
            "Epoch 2/10\n",
            "45/45 - 20s - 452ms/step - accuracy: 0.8049 - loss: 0.4379 - val_accuracy: 0.1611 - val_loss: 1.4921\n",
            "Epoch 3/10\n",
            "45/45 - 20s - 455ms/step - accuracy: 0.9465 - loss: 0.2222 - val_accuracy: 0.5139 - val_loss: 0.8256\n",
            "Epoch 4/10\n",
            "45/45 - 23s - 507ms/step - accuracy: 0.9965 - loss: 0.0884 - val_accuracy: 0.6861 - val_loss: 0.6690\n",
            "Epoch 5/10\n",
            "45/45 - 18s - 401ms/step - accuracy: 1.0000 - loss: 0.0415 - val_accuracy: 0.6778 - val_loss: 0.6960\n",
            "Epoch 6/10\n",
            "45/45 - 20s - 455ms/step - accuracy: 1.0000 - loss: 0.0202 - val_accuracy: 0.4639 - val_loss: 1.2866\n",
            "Epoch 7/10\n",
            "45/45 - 14s - 321ms/step - accuracy: 1.0000 - loss: 0.0127 - val_accuracy: 0.6056 - val_loss: 0.9921\n",
            "Epoch 8/10\n",
            "45/45 - 19s - 431ms/step - accuracy: 1.0000 - loss: 0.0086 - val_accuracy: 0.5139 - val_loss: 1.2508\n",
            "Epoch 9/10\n",
            "45/45 - 14s - 301ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 0.6083 - val_loss: 1.0459\n",
            "Epoch 10/10\n",
            "45/45 - 21s - 459ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.6111 - val_loss: 1.0870\n",
            "Test Accuracy: 67.500001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiFK3wXFAlbQ"
      },
      "source": [
        "Running the example shows better performance.\n",
        "\n",
        "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
        "\n",
        "Again, the training dataset is easily learned and the model achieves 76% accuracy on the test dataset. This is good, but not as good as using a learned Embedding layer.\n",
        "\n",
        "This may be cause of the higher quality vectors trained on more data and/or using a slightly different training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1n7HLslAmQ9"
      },
      "source": [
        "```...\n",
        "Epoch 6/10\n",
        "2s - loss: 0.0278 - acc: 1.0000\n",
        "Epoch 7/10\n",
        "2s - loss: 0.0174 - acc: 1.0000\n",
        "Epoch 8/10\n",
        "2s - loss: 0.0117 - acc: 1.0000\n",
        "Epoch 9/10\n",
        "2s - loss: 0.0086 - acc: 1.0000\n",
        "Epoch 10/10\n",
        "2s - loss: 0.0068 - acc: 1.0000\n",
        "Test Accuracy: 76.000000\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytGQc30hAoH9"
      },
      "source": [
        "In this case, it seems that learning the embedding as part of the learning task may be a better direction than using a specifically trained embedding or a more general pre-trained embedding."
      ]
    }
  ]
}
