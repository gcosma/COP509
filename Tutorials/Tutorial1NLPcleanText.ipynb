{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gcosma/COP509/blob/main/Tutorial1NLPcleanText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uedSt3uFFNSo"
      },
      "source": [
        "#**How to Clean Text for Machine Learning with Python**\n",
        "\n",
        "**Original Source:** Jason Brownlee, [How to Clean Text for Machine Learning with Python](https://machinelearningmastery.com/clean-text-machine-learning-python/), Available from [here](https://machinelearningmastery.com/clean-text-machine-learning-python/), accessed December 13, 2021.\n",
        "\n",
        "In this tutorial, you will discover how you can clean and prepare your text ready for modeling. After completing this tutorial, you will know:\n",
        "\n",
        "- How to get started by developing your own very simple text cleaning tools.\n",
        "- How to take a step up and use the more sophisticated methods in the NLTK library.\n",
        "- How to prepare text when using modern text representation methods like word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88EVcMrEFUhw"
      },
      "source": [
        "Tutorial Overview\n",
        "This tutorial is divided into 6 parts; they are:\n",
        "\n",
        "- Metamorphosis by Franz Kafka\n",
        "- Text Cleaning is Task Specific\n",
        "- Manual Tokenization\n",
        "- Tokenization and Cleaning with NLTK\n",
        "- Additional Text Cleaning Considerations\n",
        "- Tips for Cleaning Text for Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"What are the reasons for examining text data before beginning the cleaning process in natural language processing tasks?\"**\n",
        "\n",
        "**Understanding the Data:** Before any preprocessing, it's important to understand the nature of the text you're working with. This includes identifying the language, the presence of special characters, and any anomalies such as non-text elements (images, tables, etc.). Understanding these aspects can significantly influence how you approach cleaning and preprocessing the text.\n",
        "\n",
        "**Identifying Noise:** Not all elements in a text are useful for every analysis or machine learning task. By examining the text, you can identify what constitutes noise (e.g., irrelevant symbols, formatting characters, or specific types of information that are not useful for the task at hand). This helps in designing a cleaning process that removes or retains the right elements.\n",
        "\n",
        "**Preserving Meaningful Information:** Some elements that may initially appear as noise could hold meaningful information. For example, emojis in social media text can convey sentiment, and punctuation can affect the meaning of sentences. A preliminary review helps in deciding which elements are crucial for maintaining the intended meaning of the text.\n",
        "\n",
        "**Customizing Cleaning Steps:** Text data can vary widely across sources and applications, necessitating different cleaning approaches. For instance, literary texts might require preserving stylistic elements like capitalization and punctuation, while user-generated content on social media might require specialized handling of slang, abbreviations, and emojis. Pre-analysis ensures that the cleaning process is tailored to the specific characteristics of the text.\n",
        "\n",
        "**Efficiency and Effectiveness:** By understanding the text's structure and content beforehand, you can choose the most efficient tools and methods for cleaning and preprocessing, avoiding unnecessary steps that don't contribute to your analysis or model's performance. This saves time and computational resources and can lead to more accurate outcomes.\n",
        "\n",
        "**Data Integrity and Quality:** Proper initial examination helps maintain the integrity and quality of the data. It ensures that the cleaning process does not inadvertently remove or alter information that is essential for analysis, preserving the richness and nuances of the original text."
      ],
      "metadata": {
        "id": "5Sk3nOn5s-NO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq3GqBTgQ0NN"
      },
      "source": [
        "**Metamorphosis by Franz Kafka download and save instructions**\n",
        "\n",
        "In this tutorial, you will use the text from the book Metamorphosis by Franz Kafka.\n",
        "\n",
        "The full text for Metamorphosis is available for free from Project Gutenberg.\n",
        "[Metamorphosis by Franz Kafka on Project Gutenberg](https://www.gutenberg.org/ebooks/5200)\n",
        "\n",
        "ASCII text version - [Metamorphosis by Franz Kafka Plain Text UTF-8 (may need to load the page twice)](https://www.gutenberg.org/cache/epub/5200/pg5200.txt).\n",
        "\n",
        "1. Download the file (or right click on the ASCII link and save as)\n",
        "2. Place it in your current working directory with the file name “metamorphosis.txt“.\n",
        "3. Open the file and delete the header and footer information (specifically copyright and license information) and save the file as “metamorphosis_clean.txt“.\n",
        "\n",
        "The start of the clean file should look like:\n",
        "\n",
        "One morning, when Gregor ....\n",
        "\n",
        "The file should end with: And, as if in confirmation of their new dreams and good intentions, a... Poor Gregor…\n",
        "\n",
        "**Text Cleaning Is Task Specific**\n",
        "\n",
        "Take a moment to look at the text. What do you notice?\n",
        "Here are some observations:\n",
        "\n",
        "- It’s plain text so there is no markup to parse (yay!).\n",
        "- The translation of the original German uses UK English (e.g. “travelling“).\n",
        "- The lines are artificially wrapped with new lines at about 70 characters (meh).\n",
        "- There are no obvious typos or spelling mistakes.\n",
        "- There’s punctuation like commas, apostrophes, quotes, question marks, and more.\n",
        "- There’s hyphenated descriptions like “armour-like”.\n",
        "- There’s a lot of use of the em dash (“-“) to continue sentences (maybe replace with commas?).\n",
        "- There are names (e.g. “Mr. Samsa“)\n",
        "- There does not appear to be numbers that require handling (e.g. 1999)\n",
        "- There are section markers (e.g. “II” and “III”), and we have removed the first “I”.\n",
        "\n",
        "We are going to look at general text cleaning steps in this tutorial.\n",
        "\n",
        "Nevertheless, consider some possible task objectives we may have when working with this text document.\n",
        "\n",
        "For example:\n",
        "\n",
        "If we were interested in developing a Kafkaesque language model, we may want to keep all of the case, quotes, and other punctuation in place.\n",
        "If we were interested in classifying documents as “Kafka” and “Not Kafka,” maybe we would want to strip case, punctuation, and even trim words back to their stem.\n",
        "Use your task as the lens by which to choose how to ready your text data.\n",
        "\n",
        "**Manual Tokenization**\n",
        "\n",
        "Text cleaning is a difficult task, but the Metamorphosis text is quite clean already.\n",
        "\n",
        "We could just write some Python code to clean it up manually, and this is a good exercise for those simple problems that you encounter. Tools like regular expressions and splitting strings can get you a long way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjADNi1wIQRL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ec8057e-a223-49ae-f87f-ed093c1f5006"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogzIp6EwKoDC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3d9be2-b270-4acd-a837-82128685c945"
      },
      "source": [
        "!ls \"/content/drive/My Drive/Colab Notebooks/21COP509/LabDatasets/\"\n",
        "Data_path = \"/content/drive/My Drive/Colab Notebooks/21COP509/LabDatasets/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ArtsRatings_5000_test.txt   ArtsReviews_5000_train.txt\tReduced_ArtsRatings_5000.txt\n",
            "ArtsRatings_5000_train.txt  glove.6B.100d.txt\t\tReduced_ArtsReviews_5000.txt\n",
            "ArtsReviews_5000_test.txt   metamorphosis_clean.txt\treview_polarity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOLLebn_RVlS"
      },
      "source": [
        "# **1. Load Data**\n",
        "\n",
        "Let’s load the text data so that we can work with it.\n",
        "\n",
        "The text is small and will load quickly and easily fit into memory. This will not always be the case and you may need to write code to memory map the file. Tools like NLTK (covered in the next section) will make working with large files much easier.\n",
        "\n",
        "We can load the entire “metamorphosis_clean.txt” into memory as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtyaVjyrOq0l"
      },
      "source": [
        "#Load text\n",
        "file = open(Data_path + \"metamorphosis_clean.txt\",'rt')\n",
        "\n",
        "text = file.read()\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUVSAmIORZnQ"
      },
      "source": [
        "Running the example loads the whole file into memory ready to work with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZXXelBVRvP5"
      },
      "source": [
        "# **2. Split by Whitespace**\n",
        "\n",
        "Clean text often means a list of words or tokens that we can work with in our machine learning models. This means converting the raw text into a list of words and saving it again.\n",
        "\n",
        "A very simple way to do this would be to split the document by white space, including ” “, new lines, tabs and more. We can do this in Python with the split() function on the loaded string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu1bQxIKFcL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37ac2347-3e16-4698-8a78-0dd49319bffd"
      },
      "source": [
        "# split into words by white space\n",
        "words = text.split()\n",
        "print(words[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffMetamorphosis', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '“What’s', 'happened', 'to', 'me?”', 'he', 'thought.', 'It']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXdJWHjRR7H7"
      },
      "source": [
        "Running the example splits the document into a long list of words and prints the first 100 for us to review.\n",
        "\n",
        "We can see that punctuation is preserved (e.g. “wasn’t” and “armour-like“), which is nice. We can also see that end of sentence punctuation is kept with the last word (e.g. “thought.”), which is not great."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjIQ8eJASaEY"
      },
      "source": [
        "# **3. Select Words**\n",
        "\n",
        "Another approach might be to use the regex model (re) and split the document into words by selecting for strings of alphanumeric characters (a-z, A-Z, 0-9 and ‘_’)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw6Hz3ycFcWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b744c2cf-4dd3-40dd-ba0f-b9ec0d979554"
      },
      "source": [
        "# split based on words only\n",
        "import re\n",
        "words = re.split(r'\\W+', text)\n",
        "print(words[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'Metamorphosis', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjmFyL6oSrFT"
      },
      "source": [
        "Again, running the example we can see that we get our list of words. This time, we can see that “armour-like” is now two words “armour” and “like” (fine) but contractions like “What’s” is also two words “What” and “s” (not great).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpyD11dSSEPr"
      },
      "source": [
        "# **4. Split by Whitespace and Remove Punctuation**\n",
        "\n",
        "Note: This example was written for Python 3.\n",
        "\n",
        "We may want the words, but without the punctuation like commas and quotes. We also want to keep contractions together.\n",
        "\n",
        "One way would be to split the document into words by white space (as in “2. Split by Whitespace“), then use string translation to replace all punctuation with nothing (e.g. remove it).\n",
        "\n",
        "Python provides a constant called string.punctuation that provides a great list of punctuation characters. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kda7O52cFkgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2148f5-497c-42f0-cfdc-8e5aed2d3ece"
      },
      "source": [
        "import string\n",
        "print(string.punctuation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58lYs-idTBRk"
      },
      "source": [
        "Python offers a function called translate() that will map one set of characters to another.\n",
        "\n",
        "We can use the function maketrans() to create a mapping table. We can create an empty mapping table, but the third argument of this function allows us to list all of the characters to remove during the translation process. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ym9m4tLFknY"
      },
      "source": [
        "table = str.maketrans('', '', string.punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk3iInSBTEW0"
      },
      "source": [
        "We can put all of this together, load the text file, split it into words by white space, then translate each word to remove the punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4koK3pSnUtRL",
        "outputId": "bb38b801-f894-4e54-af7f-4d5ec5116e5e"
      },
      "source": [
        "# load text\n",
        "file = open(Data_path + \"metamorphosis_clean.txt\",'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into words by white space\n",
        "words = text.split()\n",
        "# remove punctuation from each word\n",
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "stripped = [w.translate(table) for w in words]\n",
        "print(stripped[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffMetamorphosis', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armourlike', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '“What’s', 'happened', 'to', 'me”', 'he', 'thought', 'It']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kb7nCrLU45N"
      },
      "source": [
        "We can see that this has had the desired effect, mostly.\n",
        "\n",
        "Contractions like “What’s” have become “Whats” but “armour-like” has become “armourlike“."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgApbGjgxAqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da8abf5-1378-4378-d44b-ca8245784f94"
      },
      "source": [
        "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armourlike', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'Whats', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasnt', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['One',\n",
              " 'morning',\n",
              " 'when',\n",
              " 'Gregor',\n",
              " 'Samsa',\n",
              " 'woke',\n",
              " 'from',\n",
              " 'troubled',\n",
              " 'dreams',\n",
              " 'he',\n",
              " 'found',\n",
              " 'himself',\n",
              " 'transformed',\n",
              " 'in',\n",
              " 'his',\n",
              " 'bed',\n",
              " 'into',\n",
              " 'a',\n",
              " 'horrible',\n",
              " 'vermin',\n",
              " 'He',\n",
              " 'lay',\n",
              " 'on',\n",
              " 'his',\n",
              " 'armourlike',\n",
              " 'back',\n",
              " 'and',\n",
              " 'if',\n",
              " 'he',\n",
              " 'lifted',\n",
              " 'his',\n",
              " 'head',\n",
              " 'a',\n",
              " 'little',\n",
              " 'he',\n",
              " 'could',\n",
              " 'see',\n",
              " 'his',\n",
              " 'brown',\n",
              " 'belly',\n",
              " 'slightly',\n",
              " 'domed',\n",
              " 'and',\n",
              " 'divided',\n",
              " 'by',\n",
              " 'arches',\n",
              " 'into',\n",
              " 'stiff',\n",
              " 'sections',\n",
              " 'The',\n",
              " 'bedding',\n",
              " 'was',\n",
              " 'hardly',\n",
              " 'able',\n",
              " 'to',\n",
              " 'cover',\n",
              " 'it',\n",
              " 'and',\n",
              " 'seemed',\n",
              " 'ready',\n",
              " 'to',\n",
              " 'slide',\n",
              " 'off',\n",
              " 'any',\n",
              " 'moment',\n",
              " 'His',\n",
              " 'many',\n",
              " 'legs',\n",
              " 'pitifully',\n",
              " 'thin',\n",
              " 'compared',\n",
              " 'with',\n",
              " 'the',\n",
              " 'size',\n",
              " 'of',\n",
              " 'the',\n",
              " 'rest',\n",
              " 'of',\n",
              " 'him',\n",
              " 'waved',\n",
              " 'about',\n",
              " 'helplessly',\n",
              " 'as',\n",
              " 'he',\n",
              " 'looked',\n",
              " 'Whats',\n",
              " 'happened',\n",
              " 'to',\n",
              " 'me',\n",
              " 'he',\n",
              " 'thought',\n",
              " 'It',\n",
              " 'wasnt',\n",
              " 'a',\n",
              " 'dream',\n",
              " 'His',\n",
              " 'room',\n",
              " 'a',\n",
              " 'proper',\n",
              " 'human']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCUrjav0xHrB"
      },
      "source": [
        "If you know anything about regex, then you know things can get complex from here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3IhSjinTLpj"
      },
      "source": [
        "# **5. Normalizing Case**\n",
        "\n",
        "It is common to convert all words to one case.\n",
        "\n",
        "This means that the vocabulary will shrink in size, but some distinctions are lost (e.g. “Apple” the company vs “apple” the fruit is a commonly used example).\n",
        "\n",
        "We can convert all words to lowercase by calling the lower() function on each word.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7HVFhrrTOYa",
        "outputId": "97d9938d-b331-4e09-9036-fab44544c263"
      },
      "source": [
        "file = open(Data_path + \"metamorphosis_clean.txt\",'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into words by white space\n",
        "words = text.split()\n",
        "# convert to lower case\n",
        "words = [word.lower() for word in words]\n",
        "print(words[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffmetamorphosis', 'by', 'franz', 'kafka', 'translated', 'by', 'david', 'wyllie', 'one', 'morning,', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'he', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'his', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '“what’s', 'happened', 'to', 'me?”', 'he', 'thought.', 'it']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HalxqphATRU8"
      },
      "source": [
        "**Note**\n",
        "\n",
        "Cleaning text is really hard, problem specific, and full of tradeoffs.\n",
        "\n",
        "Remember, simple is better.\n",
        "\n",
        "Simpler text data, simpler models, smaller vocabularies. You can always make things more complex later to see if it results in better model skill.\n",
        "\n",
        "Next, we’ll look at some of the tools in the NLTK library that offer more than simple string splitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjikXZN_TUPz"
      },
      "source": [
        "# **Tokenization and Cleaning with NLTK**\n",
        "\n",
        "The Natural Language Toolkit, or NLTK for short, is a Python library written for working and modeling text.\n",
        "\n",
        "It provides good tools for loading and cleaning text that we can use to get our data ready for working with machine learning and deep learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyd-s3acTVjL"
      },
      "source": [
        "# **1. Install NLTK**\n",
        "\n",
        "You can install NLTK using your favorite package manager, such as pip:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HkNigfhTXyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a449f7e0-701b-4818-a16d-3dceca678e4f"
      },
      "source": [
        "!pip install -U nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdWBp549TbV0"
      },
      "source": [
        "After installing the NLTK package, please do install the necessary datasets/models for specific functions to work.\n",
        "\n",
        "If you’re unsure of which datasets/models you’ll need, you can install the “popular” subset of NLTK data, on the command line type python -m nltk.downloader popular, or in the Python interpreter import nltk; nltk.download('popular')\n",
        "\n",
        "For details, see https://www.nltk.org/data.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWoyTaaLTeAz",
        "outputId": "da0605eb-cc4d-4f49-c7bf-e732ddeea8f7"
      },
      "source": [
        "import nltk; nltk.download('popular')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBRohmo6TgGF"
      },
      "source": [
        "Or from the command line:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkjj9iPOu5be"
      },
      "source": [
        "`#python -m nltk.downloader all`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ujcMHTRTlXN"
      },
      "source": [
        "# **2. Split into Sentences**\n",
        "Some modeling tasks prefer input to be in the form of paragraphs or sentences, such as word2vec. You could first split your text into sentences, split each sentence into words, then save each sentence to file, one per line.\n",
        "\n",
        "NLTK provides the sent_tokenize() function to split text into sentences.\n",
        "\n",
        "The example below loads the “metamorphosis_clean.txt” file into memory, splits it into sentences, and prints the first sentence."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('popular')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6lzeBV5bcTa",
        "outputId": "756571d5-e9ac-41aa-e9a0-032776dacf6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4J7CrPcTkFU"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#!ls \"/content/drive/My Drive/Colab Notebooks\"\n",
        "Data_path = \"/content/drive/My Drive/Colab Notebooks/21COP509/LabDatasets/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbu-5tfPYlgw",
        "outputId": "543149a6-b172-4f0b-f1d0-ed6def76b80b"
      },
      "source": [
        "# load data\n",
        "file = open(Data_path + \"metamorphosis_clean.txt\",'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into sentences\n",
        "from nltk import sent_tokenize\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿Metamorphosis\n",
            "\n",
            "by Franz Kafka\n",
            "\n",
            "Translated by David Wyllie\n",
            "\n",
            "One morning, when Gregor Samsa woke from troubled dreams, he found\n",
            "himself transformed in his bed into a horrible vermin.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ2yNNFnToVs"
      },
      "source": [
        "Running the example, we can see that although the document is split into sentences, that each sentence still preserves the new line from the artificial wrap of the lines in the original document.\n",
        "\n",
        "\"One morning, when Gregor Samsa woke from troubled dreams, he found\n",
        "himself transformed in his bed into a horrible vermin.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPcxQes-Tqwc"
      },
      "source": [
        "# **3. Split into Words**\n",
        "\n",
        "NLTK provides a function called word_tokenize() for splitting strings into tokens (nominally words).\n",
        "\n",
        "It splits tokens based on white space and punctuation. For example, commas and periods are taken as separate tokens. Contractions are split apart (e.g. “What’s” becomes “What” “‘s“). Quotes are kept, and so on.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhhH5EXeTsgi",
        "outputId": "2d71bf11-47ea-4242-ecf5-cb4630fae13d"
      },
      "source": [
        "# load data\n",
        "file = open(Data_path + \"metamorphosis_clean.txt\",'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into words\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffMetamorphosis', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', '.', 'His', 'many', 'legs', ',', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'waved', 'about', 'helplessly', 'as']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8tJ49WQTuRU"
      },
      "source": [
        "Running the code, we can see that punctuation are now tokens that we could then decide to specifically filter out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKy-BnFUTv3V"
      },
      "source": [
        "# **5. Filter out Stop Words (and Pipeline)**\n",
        "\n",
        "Stop words are those words that do not contribute to the deeper meaning of the phrase.\n",
        "\n",
        "They are the most common words such as: “the“, “a“, and “is“.\n",
        "\n",
        "For some applications like documentation classification, it may make sense to remove stop words.\n",
        "\n",
        "NLTK provides a list of commonly agreed upon stop words for a variety of languages, such as English. They can be loaded as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xis3MwRhTxUN",
        "outputId": "2f2b72fb-2882-4af0-c1d6-c60162cb2c5e"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwIqszw3Tzk9"
      },
      "source": [
        "You can see the full list as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62esLAXnT1D-"
      },
      "source": [
        "You can see that they are all lower case and have punctuation removed.\n",
        "\n",
        "You could compare your tokens to the stop words and filter them out, but you must ensure that your text is prepared the same way.\n",
        "\n",
        "**Let’s demonstrate this with a small pipeline of text preparation including:**\n",
        "1. Load the raw text.\n",
        "2. Split into tokens.\n",
        "3. Convert to lowercase.\n",
        "4. Remove punctuation from each token.\n",
        "5. Filter out remaining tokens that are not alphabetic.\n",
        "6. Filter out tokens that are stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsKR2vv6T5tV",
        "outputId": "704d0744-6690-4dd9-e3e3-e3c04e7af5eb"
      },
      "source": [
        "# load data\n",
        "file = open(Data_path + \"metamorphosis_clean.txt\",'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into words\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "# convert to lower case\n",
        "tokens = [w.lower() for w in tokens]\n",
        "# remove punctuation from each word\n",
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "stripped = [w.translate(table) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "words = [word for word in stripped if word.isalpha()]\n",
        "# filter out stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [w for w in words if not w in stop_words]\n",
        "print(words[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['franz', 'kafka', 'translated', 'david', 'wyllie', 'one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed', 'ready', 'slide', 'moment', 'many', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 'happened', 'thought', 'dream', 'room', 'proper', 'human', 'room', 'although', 'little', 'small', 'lay', 'peacefully', 'four', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'travelling', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine', 'housed', 'nice', 'gilded', 'frame', 'showed', 'lady', 'fitted', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'raising', 'heavy', 'fur', 'muff', 'covered', 'whole', 'lower', 'arm', 'towards']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwPHyK7aT8md"
      },
      "source": [
        "Running this example, we can see that in addition to all of the other transforms, stop words like “a” and “to” have been removed.\n",
        "\n",
        "I note that we are still left with tokens like “nt“. The rabbit hole is deep; there’s always more we can do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpZhAY1IT_BW"
      },
      "source": [
        "# **6. Stem Words**\n",
        "\n",
        "Stemming refers to the process of reducing each word to its root or base.\n",
        "\n",
        "For example “fishing,” “fished,” “fisher” all reduce to the stem “fish.”\n",
        "\n",
        "Some applications, like document classification, may benefit from stemming in order to both reduce the vocabulary and to focus on the sense or sentiment of a document rather than deeper meaning.\n",
        "\n",
        "There are many stemming algorithms, although a popular and long-standing method is the Porter Stemming algorithm. This method is available in NLTK via the PorterStemmer class.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgdgdg1bUArF",
        "outputId": "36b1e2ec-39bf-444f-af0b-205328180508"
      },
      "source": [
        "# load data\n",
        "file = open(Data_path + \"metamorphosis_clean.txt\",'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into words\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "# stemming of words\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "stemmed = [porter.stem(word) for word in tokens]\n",
        "print(stemmed[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffmetamorphosi', 'by', 'franz', 'kafka', 'translat', 'by', 'david', 'wylli', 'one', 'morn', ',', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubl', 'dream', ',', 'he', 'found', 'himself', 'transform', 'in', 'hi', 'bed', 'into', 'a', 'horribl', 'vermin', '.', 'he', 'lay', 'on', 'hi', 'armour-lik', 'back', ',', 'and', 'if', 'he', 'lift', 'hi', 'head', 'a', 'littl', 'he', 'could', 'see', 'hi', 'brown', 'belli', ',', 'slightli', 'dome', 'and', 'divid', 'by', 'arch', 'into', 'stiff', 'section', '.', 'the', 'bed', 'wa', 'hardli', 'abl', 'to', 'cover', 'it', 'and', 'seem', 'readi', 'to', 'slide', 'off', 'ani', 'moment', '.', 'hi', 'mani', 'leg', ',', 'piti', 'thin', 'compar', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'wave', 'about', 'helplessli', 'as']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX-6ApTOUAMu"
      },
      "source": [
        "Running the example, you can see that words have been reduced to their stems, such as “trouble” has become “troubl“. You can also see that the stemming implementation has also reduced the tokens to lowercase, likely for internal look-ups in word tables.\n",
        "\n",
        "You can also see that the stemming implementation has also reduced the tokens to lowercase, likely for internal look-ups in word tables.\n",
        "\n",
        "There is a nice suite of stemming and lemmatization algorithms to choose from in NLTK, if reducing words to their root is something you need for your project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN-Cl1bPUGdu"
      },
      "source": [
        "#**Additional Text Cleaning Considerations**\n",
        "We are only getting started.\n",
        "\n",
        "Because the source text for this tutorial was reasonably clean to begin with, we skipped many concerns of text cleaning that you may need to deal with in your own project.\n",
        "\n",
        "Here is a short list of additional considerations when cleaning text:\n",
        "\n",
        "- Handling large documents and large collections of text documents that do not fit into memory.\n",
        "- Extracting text from markup like HTML, PDF, or other structured document formats.\n",
        "- Transliteration of characters from other languages into English.\n",
        "- Decoding Unicode characters into a normalized form, such as UTF8.\n",
        "- Handling of domain specific words, phrases, and acronyms.\n",
        "- Handling or removing numbers, such as dates and amounts.\n",
        "- Locating and correcting common typos and misspellings.\n",
        "…\n",
        "The list could go on.\n",
        "\n",
        "Ideally, you would save a new file after each transform so that you can spend time with all of the data in the new form. Things always jump out at you when to take the time to review your data.\n",
        "\n",
        "#**Tips for Cleaning Text for Word Embedding**\n",
        "\n",
        "Recently, the field of natural language processing has been moving away from bag-of-word models and word encoding toward word embeddings.\n",
        "\n",
        "The benefit of word embeddings is that they encode each word into a dense vector that captures something about its relative meaning within the training text.\n",
        "\n",
        "This means that variations of words like case, spelling, punctuation, and so on will automatically be learned to be similar in the embedding space. In turn, this can mean that the amount of cleaning required from your text may be less and perhaps quite different to classical text cleaning.\n",
        "\n",
        "For example, it may no-longer make sense to stem words or remove punctuation for contractions.\n",
        "\n",
        "Tomas Mikolov is one of the developers of word2vec, a popular word embedding method. He suggests only very minimal text cleaning is required when learning a word embedding model.\n",
        "\n",
        "Below is his response when pressed with the question about how to best prepare text data for word2vec.\n",
        "\n",
        "*There is no universal answer. It all depends on what you plan to use the vectors for. In my experience, it is usually good to disconnect (or remove) punctuation from words, and sometimes also convert all characters to lowercase. One can also replace all numbers (possibly greater than some constant) with some single token such as.\n",
        "All these pre-processing steps aim to reduce the vocabulary size without removing any important content (which in some cases may not be true when you lowercase certain words, ie. ‘Bush’ is different than ‘bush’, while ‘Another’ has usually the same sense as ‘another’). The smaller the vocabulary is, the lower is the memory complexity, and the more robustly are the parameters for the words estimated. You also have to pre-process the test data in the same way.\n",
        "…\n",
        "In short, you will understand all this much better if you will run experiments.*\n",
        "\n",
        "**Specifically, you learned:**\n",
        "\n",
        "- How to get started by developing your own very simple text cleaning tools.\n",
        "\n",
        "- How to take a step up and use the more sophisticated methods in the NLTK library.\n",
        "\n",
        "- How to prepare text when using modern text representation methods like word embeddings.\n",
        "\n",
        "**Original Source:** Jason Brownlee, [How to Clean Text for Machine Learning with Python](https://machinelearningmastery.com/clean-text-machine-learning-python/), Available from [here](https://machinelearningmastery.com/clean-text-machine-learning-python/), accessed December 13, 2021.\n",
        "\n"
      ]
    }
  ]
}
