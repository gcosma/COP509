{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gcosma/COP509/blob/main/TutorialRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQLfW1PDJfEI"
      },
      "source": [
        "# COP509: Natural Language Processing\n",
        "# Tutorial: Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "**Department of Computer Science, Loughborough University**\n",
        "\n",
        "**Prof. Georgina Cosma** (g.cosma@lboro.ac.uk)\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Outcomes\n",
        "\n",
        "By the end of this tutorial, you will be able to:\n",
        "\n",
        "1. Explain what RAG is and why it addresses key limitations of large language models\n",
        "2. Implement the retrieval component using vector similarity search\n",
        "3. Understand how retrieved context is integrated with LLM prompts\n",
        "4. Build a complete RAG pipeline in Python\n",
        "5. Evaluate RAG systems and identify common failure modes\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Prerequisites\n",
        "\n",
        "This tutorial builds on concepts from:\n",
        "- **Weeks 2-3**: Vector Space Models (TF-IDF, word embeddings)\n",
        "- **Week 4**: Similarity measures (cosine similarity)\n",
        "- **Week 5**: LSI and semantic similarity\n",
        "- **Week 6**: Transformer models (BERT, attention mechanism)\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ How to Use This Notebook\n",
        "\n",
        "| Symbol | Meaning |\n",
        "|--------|--------|\n",
        "| üìù | Important concept - read carefully |\n",
        "| üíª | Code to run |\n",
        "| ‚úÖ | Exercise for you to complete |\n",
        "| ‚ö†Ô∏è | Common mistake or warning |\n",
        "| üîë | Key takeaway |\n",
        "\n",
        "**Instructions:**\n",
        "1. **Run cells in order** - each cell depends on previous cells\n",
        "2. **Read the explanations** - don't just run the code!\n",
        "3. **Check expected outputs** - verify your results match\n",
        "4. **Complete the exercises** - practice is essential for learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIhaKUoKJfEK"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 1: Setup and Installation\n",
        "\n",
        "### üìù What are we installing?\n",
        "\n",
        "| Library | Purpose | Why we need it |\n",
        "|---------|---------|----------------|\n",
        "| `sentence-transformers` | Creates dense embeddings | Converts text to vectors that capture meaning |\n",
        "| `chromadb` | Vector database | Stores and searches embeddings efficiently |\n",
        "| `langchain` | LLM framework | Provides tools to build RAG pipelines |\n",
        "| `transformers` | Pre-trained models | Gives us access to language models like FLAN-T5 |\n",
        "\n",
        "**‚è±Ô∏è Note:** Installation takes 2-3 minutes. You only need to run this once per Colab session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFBOpZLgJfEL",
        "outputId": "948ea422-fa0e-4c52-bd6b-0f8026a4540e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing packages... this may take 2-3 minutes...\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# üíª STEP 1: Install required packages\n",
        "# The -q flag means 'quiet' - it reduces output clutter\n",
        "print(\"üì¶ Installing packages... this may take 2-3 minutes...\")\n",
        "!pip install -q sentence-transformers chromadb langchain-text-splitters\n",
        "!pip install -q transformers accelerate\n",
        "print(\"‚úÖ Installation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1irJ1FYJfEL"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 2: Import libraries\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRJq7nSVJfEM"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "‚úÖ Libraries imported successfully!\n",
        "```\n",
        "\n",
        "**‚ö†Ô∏è Troubleshooting:** If you see errors:\n",
        "- Try running the installation cell again\n",
        "- Go to **Runtime ‚Üí Restart runtime**, then run both cells again\n",
        "- Make sure you're connected to the internet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCTwKxNhJfEM"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Understanding the Problem\n",
        "\n",
        "### üìù What is an LLM?\n",
        "\n",
        "**LLM** = **L**arge **L**anguage **M**odel\n",
        "\n",
        "These are neural networks trained on massive amounts of text to understand and generate human language.\n",
        "\n",
        "**Examples:** ChatGPT, Claude, BERT, GPT-4, FLAN-T5\n",
        "\n",
        "### üìù The Problem: Why LLMs Need Help\n",
        "\n",
        "LLMs have three major limitations:\n",
        "\n",
        "| Limitation | What it means | Example |\n",
        "|------------|--------------|--------|\n",
        "| **Knowledge Cutoff** | Only knows information from training data | Cannot answer \"Who won the 2025 election?\" |\n",
        "| **Hallucinations** | Generates plausible but FALSE information | May invent statistics or citations |\n",
        "| **No Private Data** | Cannot access your documents | Cannot answer \"What is our company's leave policy?\" |\n",
        "\n",
        "### üìù The Solution: RAG\n",
        "\n",
        "**RAG** = **R**etrieval-**A**ugmented **G**eneration\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                        RAG PIPELINE                             ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                 ‚îÇ\n",
        "‚îÇ   User Query ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                           ‚îÇ\n",
        "‚îÇ                    ‚îÇ                                           ‚îÇ\n",
        "‚îÇ                    ‚ñº                                           ‚îÇ\n",
        "‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ\n",
        "‚îÇ            ‚îÇ   RETRIEVE   ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ  Relevant Docs   ‚îÇ         ‚îÇ\n",
        "‚îÇ            ‚îÇ   (Search)   ‚îÇ      ‚îÇ  from Database   ‚îÇ         ‚îÇ\n",
        "‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n",
        "‚îÇ                                           ‚îÇ                    ‚îÇ\n",
        "‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ\n",
        "‚îÇ                    ‚ñº                                           ‚îÇ\n",
        "‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                    ‚îÇ\n",
        "‚îÇ            ‚îÇ   AUGMENT    ‚îÇ  Query + Retrieved Docs            ‚îÇ\n",
        "‚îÇ            ‚îÇ   (Combine)  ‚îÇ  = Augmented Prompt                ‚îÇ\n",
        "‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                    ‚îÇ\n",
        "‚îÇ                   ‚îÇ                                            ‚îÇ\n",
        "‚îÇ                   ‚ñº                                            ‚îÇ\n",
        "‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                    ‚îÇ\n",
        "‚îÇ            ‚îÇ   GENERATE   ‚îÇ                                    ‚îÇ\n",
        "‚îÇ            ‚îÇ    (LLM)     ‚îÇ                                    ‚îÇ\n",
        "‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                    ‚îÇ\n",
        "‚îÇ                   ‚îÇ                                            ‚îÇ\n",
        "‚îÇ                   ‚ñº                                            ‚îÇ\n",
        "‚îÇ              Answer grounded                                   ‚îÇ\n",
        "‚îÇ              in real documents                                 ‚îÇ\n",
        "‚îÇ                                                                ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### üîë Key Analogy\n",
        "\n",
        "| Standard LLM | RAG |\n",
        "|--------------|-----|\n",
        "| Closed-book exam | Open-book exam |\n",
        "| Must answer from memory | Can look up information |\n",
        "| May guess incorrectly | Answers grounded in sources |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WiNXotMJfEM"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: The Retrieval Component\n",
        "\n",
        "### üìù What is Retrieval?\n",
        "\n",
        "**Retrieval** = Finding relevant documents for a given query\n",
        "\n",
        "This is exactly what you learned in **Weeks 2-5**! The process is:\n",
        "\n",
        "```\n",
        "Step 1: Convert all documents into vectors (numbers)\n",
        "        ‚Üì\n",
        "Step 2: Convert the user's query into a vector\n",
        "        ‚Üì\n",
        "Step 3: Calculate similarity between query and all documents\n",
        "        ‚Üì\n",
        "Step 4: Return the top-k most similar documents\n",
        "```\n",
        "\n",
        "### üìù What is \"top-k\"?\n",
        "\n",
        "**k** is simply the number of documents to retrieve:\n",
        "- **k=3** means retrieve the 3 most similar documents\n",
        "- **k=5** means retrieve the 5 most similar documents\n",
        "\n",
        "**Trade-off:** Higher k = more context, but may include irrelevant documents\n",
        "\n",
        "### üìù Dense vs Sparse Embeddings\n",
        "\n",
        "| Type | Method | What it looks like | Pros | Cons |\n",
        "|------|--------|-------------------|------|------|\n",
        "| **Sparse** | TF-IDF (Week 2-3) | `[0, 0, 0, 1, 0, 0, ..., 0]` | Fast, interpretable | Only matches exact words |\n",
        "| **Dense** | Neural embeddings | `[0.2, -0.5, 0.8, ..., 0.1]` | Captures meaning | Requires more computation |\n",
        "\n",
        "### üîë Why Dense Embeddings are Better for RAG\n",
        "\n",
        "**Example:** User asks \"How many holiday days do I get?\"\n",
        "\n",
        "| Method | Can it find \"Annual leave is 30 days\"? | Why? |\n",
        "|--------|---------------------------------------|------|\n",
        "| TF-IDF (Sparse) | ‚ùå Low score | \"holiday\" ‚â† \"annual leave\" (different words) |\n",
        "| Dense Embeddings | ‚úÖ High score | Understands \"holiday\" ‚âà \"annual leave\" (same meaning) |\n",
        "\n",
        "### üìù What is Sentence-Transformers?\n",
        "\n",
        "**Sentence-Transformers** is a library that provides pre-trained models to convert text into dense vectors.\n",
        "\n",
        "We'll use `all-MiniLM-L6-v2` which:\n",
        "- ‚úÖ Is **free** and runs locally (no API key!)\n",
        "- ‚úÖ Produces **384-dimensional** vectors\n",
        "- ‚úÖ Is **fast** and works well for most tasks\n",
        "- ‚úÖ Understands **semantic similarity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiKh9-84JfEN"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 3: Load the embedding model\n",
        "# First run downloads ~90MB - be patient!\n",
        "print(\"üì• Loading embedding model (first run downloads ~90MB)...\")\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"‚úÖ Model loaded!\")\n",
        "print(f\"   This model produces {embedding_model.get_sentence_embedding_dimension()}-dimensional vectors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlyonrlXJfEN"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "üì• Loading embedding model (first run downloads ~90MB)...\n",
        "‚úÖ Model loaded!\n",
        "   This model produces 384-dimensional vectors\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nHjoa0rJfEN"
      },
      "source": [
        "### üìù Step 3.1: Create a Knowledge Base\n",
        "\n",
        "In a real application, you would load documents from files. For this tutorial, we'll create a simple knowledge base of company policies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk9MeDTUJfEN"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 4: Create our knowledge base\n",
        "# These are example company policy documents\n",
        "documents = [\n",
        "    \"Annual leave allowance is 30 days per year for all full-time employees.\",\n",
        "    \"Sick leave requires a doctor's note after 3 consecutive days of absence.\",\n",
        "    \"Remote working is permitted 2 days per week with manager approval.\",\n",
        "    \"The office dress code is business casual from Monday to Thursday.\",\n",
        "    \"Friday is casual dress day and employees may wear jeans.\",\n",
        "    \"All employees must complete mandatory health and safety training annually.\",\n",
        "    \"Parental leave is 26 weeks for primary carers and 4 weeks for secondary carers.\",\n",
        "    \"The company provides a pension contribution of 5% of salary.\",\n",
        "    \"Performance reviews are conducted twice per year in March and September.\",\n",
        "    \"Expenses must be submitted within 30 days of being incurred.\"\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ Knowledge base created with {len(documents)} documents\")\n",
        "print(f\"\\nüìÑ Example document: '{documents[0]}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERXCmUqfJfEN"
      },
      "source": [
        "### üìù Step 3.2: Convert Documents to Embeddings\n",
        "\n",
        "Now we convert each document into a vector. This is called **indexing**.\n",
        "\n",
        "**What happens:**\n",
        "```\n",
        "\"Annual leave is 30 days...\"  ‚Üí  [0.023, -0.156, 0.089, ..., 0.045]\n",
        "                                        (384 numbers)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8wMuYiHJfEN"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 5: Create embeddings for all documents\n",
        "# This is the INDEXING step - done ONCE before any queries\n",
        "print(\"üîÑ Creating document embeddings...\")\n",
        "doc_embeddings = embedding_model.encode(documents)\n",
        "\n",
        "print(f\"‚úÖ Created embeddings!\")\n",
        "print(f\"   Shape: {doc_embeddings.shape}\")\n",
        "print(f\"   Meaning: {doc_embeddings.shape[0]} documents √ó {doc_embeddings.shape[1]} dimensions\")\n",
        "print(f\"\\nüìä First 5 values of document 1's embedding:\")\n",
        "print(f\"   {doc_embeddings[0][:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFtzV651JfEN"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "üîÑ Creating document embeddings...\n",
        "‚úÖ Created embeddings!\n",
        "   Shape: (10, 384)\n",
        "   Meaning: 10 documents √ó 384 dimensions\n",
        "\n",
        "üìä First 5 values of document 1's embedding:\n",
        "   [ 0.01234  -0.05678  0.12345  -0.09876  0.03456]\n",
        "```\n",
        "(Your exact numbers will be slightly different)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQGUrOkXJfEN"
      },
      "source": [
        "### üìù Step 3.3: Create the Retrieval Function\n",
        "\n",
        "Now we create a function that:\n",
        "1. Takes a query\n",
        "2. Converts it to an embedding\n",
        "3. Finds the most similar documents\n",
        "\n",
        "**This uses cosine similarity from Week 4!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYzFRbefJfEN"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 6: Define the retrieval function\n",
        "def retrieve_documents(query, doc_embeddings, documents, embedding_model, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieve the most relevant documents for a query.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    query : str\n",
        "        The user's question\n",
        "    doc_embeddings : numpy array\n",
        "        Pre-computed embeddings for all documents\n",
        "    documents : list\n",
        "        List of original document texts\n",
        "    embedding_model : SentenceTransformer\n",
        "        The model used to create embeddings\n",
        "    top_k : int\n",
        "        Number of documents to retrieve (default: 3)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    list of tuples: [(document_text, similarity_score), ...]\n",
        "    \"\"\"\n",
        "    # Step A: Convert query to embedding (MUST use same model!)\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "\n",
        "    # Step B: Calculate cosine similarity with ALL documents\n",
        "    similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
        "\n",
        "    # Step C: Get indices of top-k most similar documents\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    # Step D: Return documents with their scores\n",
        "    results = [(documents[i], similarities[i]) for i in top_indices]\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Retrieval function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SrsUXCTJfEO"
      },
      "source": [
        "### üíª Let's Test the Retrieval!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hE1RuCPJfEO"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 7: Test the retrieval function\n",
        "query = \"How many holiday days do I get?\"\n",
        "\n",
        "print(f\"üîç Query: '{query}'\")\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"üìÑ Retrieved documents (ranked by similarity):\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "results = retrieve_documents(query, doc_embeddings, documents, embedding_model, top_k=3)\n",
        "\n",
        "for i, (doc, score) in enumerate(results, 1):\n",
        "    print(f\"\\n[{i}] Similarity: {score:.3f}\")\n",
        "    print(f\"    üìÑ {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9U_28r3JfEO"
      },
      "source": [
        "### üîë Key Observation!\n",
        "\n",
        "The query asked about **\"holiday days\"** but the top result mentions **\"annual leave\"**.\n",
        "\n",
        "**This is the power of dense embeddings!** They understand that:\n",
        "- \"holiday\" ‚âà \"annual leave\"\n",
        "- \"days\" ‚âà \"days\"\n",
        "\n",
        "TF-IDF would have given a LOW score because the exact words don't match!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVA5IbHPJfEO"
      },
      "source": [
        "### ‚úÖ Exercise 3.1: Test Semantic Matching\n",
        "\n",
        "Run the cell below and observe how the system finds relevant documents even when you use different words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7gQRJBbJfEO"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ EXERCISE: Observe semantic matching in action\n",
        "test_queries = [\n",
        "    \"What should I wear to the office?\",      # Should match ‚Üí dress code\n",
        "    \"Can I work from home?\",                   # Should match ‚Üí remote working\n",
        "    \"When is my performance evaluation?\",      # Should match ‚Üí performance reviews\n",
        "    \"What happens if I'm sick?\"                # Should match ‚Üí sick leave\n",
        "]\n",
        "\n",
        "print(\"Testing semantic matching:\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "for query in test_queries:\n",
        "    results = retrieve_documents(query, doc_embeddings, documents, embedding_model, top_k=1)\n",
        "    doc, score = results[0]\n",
        "    print(f\"\\nüîç Query: '{query}'\")\n",
        "    print(f\"   ‚Üí Best match [{score:.3f}]: {doc[:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGNew2f6JfEO"
      },
      "source": [
        "### ‚úÖ Exercise 3.2: Compare Sparse vs Dense\n",
        "\n",
        "Let's see how TF-IDF (sparse) compares to dense embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLRUKptoJfEO"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ EXERCISE: Compare TF-IDF vs Dense Embeddings\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create TF-IDF vectors (sparse - like Week 2-3)\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(documents)\n",
        "\n",
        "# Query where words DON'T match exactly\n",
        "query = \"How many holiday days do I get?\"\n",
        "\n",
        "# TF-IDF retrieval\n",
        "query_tfidf = tfidf.transform([query])\n",
        "tfidf_scores = cosine_similarity(query_tfidf, tfidf_matrix)[0]\n",
        "tfidf_top = np.argmax(tfidf_scores)\n",
        "\n",
        "# Dense retrieval\n",
        "dense_results = retrieve_documents(query, doc_embeddings, documents, embedding_model, top_k=1)\n",
        "\n",
        "print(f\"üîç Query: '{query}'\")\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"TF-IDF (Sparse) - matches exact words only:\")\n",
        "print(f\"   Score: {tfidf_scores[tfidf_top]:.3f}\")\n",
        "print(f\"   Doc: {documents[tfidf_top][:50]}...\")\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"Dense Embeddings - understands meaning:\")\n",
        "print(f\"   Score: {dense_results[0][1]:.3f}\")\n",
        "print(f\"   Doc: {dense_results[0][0][:50]}...\")\n",
        "print(\"\\nüîë Notice: Dense embeddings give a HIGHER score because they\")\n",
        "print(\"   understand that 'holiday' ‚âà 'annual leave'!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "985fdy9mJfEO"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: The Generation Component\n",
        "\n",
        "### üìù What is Generation?\n",
        "\n",
        "After retrieving relevant documents, we need to **generate an answer** using an LLM.\n",
        "\n",
        "**The key idea:** We don't just send the query to the LLM. We send the query **PLUS** the retrieved documents.\n",
        "\n",
        "```\n",
        "Standard LLM:     Query ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ LLM ‚îÄ‚îÄ‚ñ∫ Answer (may hallucinate)\n",
        "\n",
        "RAG:              Query + Retrieved Documents ‚îÄ‚îÄ‚ñ∫ LLM ‚îÄ‚îÄ‚ñ∫ Grounded Answer ‚úì\n",
        "```\n",
        "\n",
        "### üìù What is Prompt Augmentation?\n",
        "\n",
        "**Augmentation** = Adding retrieved documents to the prompt\n",
        "\n",
        "We create a prompt that says:\n",
        "1. \"Here is some context\" (the retrieved documents)\n",
        "2. \"Here is the question\"\n",
        "3. \"Answer based ONLY on the context\" (reduces hallucinations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azaKLseIJfEO"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 8: Create the prompt augmentation function\n",
        "def create_rag_prompt(query, retrieved_docs):\n",
        "    \"\"\"\n",
        "    Create a prompt that includes retrieved context.\n",
        "    This is the AUGMENTATION step.\n",
        "    \"\"\"\n",
        "    # Format the retrieved documents\n",
        "    context_parts = []\n",
        "    for i, (doc, score) in enumerate(retrieved_docs, 1):\n",
        "        context_parts.append(f\"[Document {i}] {doc}\")\n",
        "\n",
        "    context = \"\\n\".join(context_parts)\n",
        "\n",
        "    # Create the prompt with clear instructions\n",
        "    prompt = f\"\"\"Answer the question based ONLY on the following context.\n",
        "If the context doesn't contain enough information, say \"I don't have enough information.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "print(\"‚úÖ Prompt augmentation function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ycbo6LfDJfEO"
      },
      "outputs": [],
      "source": [
        "# üíª Let's see what an augmented prompt looks like\n",
        "query = \"How many holiday days do I get?\"\n",
        "retrieved = retrieve_documents(query, doc_embeddings, documents, embedding_model, top_k=2)\n",
        "\n",
        "prompt = create_rag_prompt(query, retrieved)\n",
        "\n",
        "print(\"üìù Generated RAG Prompt:\")\n",
        "print(\"=\"*65)\n",
        "print(prompt)\n",
        "print(\"=\"*65)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCV0Hza6JfEP"
      },
      "source": [
        "### üìù Loading the Language Model\n",
        "\n",
        "We'll use **FLAN-T5**, a free, open-source model from Google:\n",
        "- ‚úÖ Runs locally on Colab (no API key needed)\n",
        "- ‚úÖ Good at following instructions\n",
        "- ‚úÖ Small enough to run without a GPU\n",
        "\n",
        "**‚è±Ô∏è Note:** First run downloads ~1GB. This takes 2-3 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXdQB1w7JfEP"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 9: Load the language model\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "print(\"üì• Loading language model (first run downloads ~1GB)...\")\n",
        "print(\"‚è±Ô∏è  This may take 2-3 minutes...\")\n",
        "\n",
        "model_name = \"google/flan-t5-base\"  # Free, no API key needed!\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=100\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Language model loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw2nRua9JfEP"
      },
      "source": [
        "### üìù The Complete RAG Pipeline\n",
        "\n",
        "Now let's put it all together:\n",
        "\n",
        "```\n",
        "User Query\n",
        "    ‚îÇ\n",
        "    ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  1. RETRIEVE    ‚îÇ  ‚Üê Find relevant documents\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ\n",
        "         ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  2. AUGMENT     ‚îÇ  ‚Üê Add documents to prompt\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ\n",
        "         ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  3. GENERATE    ‚îÇ  ‚Üê LLM produces answer\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ\n",
        "         ‚ñº\n",
        "   Grounded Answer\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58a8gFlnJfEP"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 10: Define the complete RAG pipeline\n",
        "def rag_answer(query, doc_embeddings, documents, embedding_model, generator, top_k=3):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline: Retrieve ‚Üí Augment ‚Üí Generate\n",
        "    \"\"\"\n",
        "    # Step 1: RETRIEVE relevant documents\n",
        "    retrieved = retrieve_documents(query, doc_embeddings, documents, embedding_model, top_k)\n",
        "\n",
        "    # Step 2: AUGMENT - create prompt with context\n",
        "    prompt = create_rag_prompt(query, retrieved)\n",
        "\n",
        "    # Step 3: GENERATE answer using the LLM\n",
        "    response = generator(prompt)[0]['generated_text']\n",
        "\n",
        "    return response, retrieved\n",
        "\n",
        "print(\"‚úÖ Complete RAG pipeline defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmBbf0p8JfEP"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 11: Test the complete RAG system!\n",
        "query = \"How many holiday days do I get?\"\n",
        "\n",
        "print(f\"üîç Question: {query}\")\n",
        "print(\"\\nüîÑ Processing...\")\n",
        "\n",
        "answer, sources = rag_answer(query, doc_embeddings, documents, embedding_model, generator)\n",
        "\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(f\"üí¨ Answer: {answer}\")\n",
        "print(\"=\"*65)\n",
        "print(\"\\nüìö Sources used:\")\n",
        "for doc, score in sources:\n",
        "    print(f\"   [{score:.3f}] {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QynEoX9eJfEP"
      },
      "source": [
        "### üîë Key Observation\n",
        "\n",
        "The answer **\"30 days\"** comes directly from the retrieved document!\n",
        "\n",
        "The LLM didn't hallucinate - it extracted the information from the context we provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSBhHWVEJfEP"
      },
      "source": [
        "### ‚úÖ Exercise 4.1: Test with Different Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZj6uC2PJfEP"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ EXERCISE: Test the RAG system with multiple questions\n",
        "test_questions = [\n",
        "    \"What is the dress code on Friday?\",\n",
        "    \"How long is parental leave for primary carers?\",\n",
        "    \"When do I need a doctor's note?\",\n",
        "    \"Can I work from home?\"\n",
        "]\n",
        "\n",
        "print(\"Testing RAG system:\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "for q in test_questions:\n",
        "    answer, _ = rag_answer(q, doc_embeddings, documents, embedding_model, generator)\n",
        "    print(f\"\\n‚ùì Q: {q}\")\n",
        "    print(f\"üí¨ A: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urUsiQwMJfEP"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 5: Chunking - Splitting Long Documents\n",
        "\n",
        "### üìù What is Chunking?\n",
        "\n",
        "**Chunking** = Splitting long documents into smaller pieces\n",
        "\n",
        "### üìù Why Do We Need Chunking?\n",
        "\n",
        "| Reason | Explanation |\n",
        "|--------|-------------|\n",
        "| **Embedding model limits** | Models can only process ~512 tokens at once |\n",
        "| **LLM context limits** | Can't fit entire documents in the prompt |\n",
        "| **Precision** | We want specific paragraphs, not whole documents |\n",
        "\n",
        "### üìù Chunk Size Trade-offs\n",
        "\n",
        "```\n",
        "Chunks TOO SMALL                    Chunks TOO LARGE\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "‚ùå Lose context                      ‚ùå Include irrelevant info\n",
        "‚ùå May split sentences               ‚ùå Less precise retrieval\n",
        "‚ùå \"Annual leave is\" / \"30 days\"    ‚ùå Whole handbook in one chunk\n",
        "```\n",
        "\n",
        "**üîë Goal:** Each chunk should contain ONE complete, coherent piece of information.\n",
        "\n",
        "### üìù What is Overlap?\n",
        "\n",
        "**Overlap** = Shared text between adjacent chunks\n",
        "\n",
        "```\n",
        "Without overlap:  [Chunk 1: \"Annual leave is\"] [Chunk 2: \"30 days per year\"]\n",
        "                                           ‚Üë\n",
        "                                  Information split! ‚ùå\n",
        "\n",
        "With overlap:     [Chunk 1: \"Annual leave is 30 days\"]\n",
        "                                       [Chunk 2: \"is 30 days per year\"]\n",
        "                                           ‚Üë\n",
        "                                  Information preserved! ‚úì\n",
        "```\n",
        "\n",
        "**Rule of thumb:** Set overlap to 10-20% of chunk size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62f0nhm4JfEP"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 12: Chunking example\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# A longer document to demonstrate chunking\n",
        "long_document = \"\"\"\n",
        "EMPLOYEE HANDBOOK - LEAVE POLICIES\n",
        "\n",
        "Section 1: Annual Leave\n",
        "All full-time employees are entitled to 30 days of annual leave per year.\n",
        "Leave must be requested at least 2 weeks in advance for periods longer than 3 days.\n",
        "Up to 5 days of unused leave may be carried over to the following year.\n",
        "\n",
        "Section 2: Sick Leave\n",
        "Employees may take sick leave when unwell without a doctor's note for up to 3 days.\n",
        "For absences longer than 3 days, a medical certificate is required.\n",
        "The company provides full pay for the first 10 days of sick leave per year.\n",
        "\n",
        "Section 3: Parental Leave\n",
        "Primary carers are entitled to 26 weeks of paid parental leave.\n",
        "Secondary carers are entitled to 4 weeks of paid parental leave.\n",
        "Parental leave must be taken within the first year of the child's birth.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"üìÑ Original document length: {len(long_document)} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3kKhipDJfEP"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 13: Split the document into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,      # Maximum characters per chunk\n",
        "    chunk_overlap=50,    # Overlap between chunks\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]  # Try these in order\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_text(long_document)\n",
        "\n",
        "print(f\"‚úÖ Split into {len(chunks)} chunks\\n\")\n",
        "print(\"=\"*65)\n",
        "for i, chunk in enumerate(chunks, 1):\n",
        "    print(f\"\\nüìÑ Chunk {i} ({len(chunk)} chars):\")\n",
        "    print(\"-\"*40)\n",
        "    print(chunk.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcXwR7u_JfEP"
      },
      "source": [
        "### üìù How RecursiveCharacterTextSplitter Works\n",
        "\n",
        "It tries separators **in order**:\n",
        "\n",
        "1. First: `\\n\\n` (paragraph breaks) - keeps paragraphs together\n",
        "2. Then: `\\n` (line breaks) - keeps lines together  \n",
        "3. Then: `. ` (sentences) - keeps sentences together\n",
        "4. Finally: ` ` (spaces) - splits words as last resort\n",
        "\n",
        "This preserves natural text structure!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohgSwyenJfEP"
      },
      "source": [
        "### ‚úÖ Exercise 5.1: Experiment with Chunk Sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-A2SSl9JfET"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ EXERCISE: Try different chunk sizes\n",
        "chunk_sizes = [100, 200, 300, 500]\n",
        "\n",
        "print(\"Comparing chunk sizes:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for size in chunk_sizes:\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=size,\n",
        "        chunk_overlap=size // 10  # 10% overlap\n",
        "    )\n",
        "    result_chunks = splitter.split_text(long_document)\n",
        "    avg_len = np.mean([len(c) for c in result_chunks])\n",
        "\n",
        "    print(f\"\\nChunk size {size}:\")\n",
        "    print(f\"   ‚Üí {len(result_chunks)} chunks (avg {avg_len:.0f} chars each)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6QnQR3tJfET"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Vector Databases\n",
        "\n",
        "### üìù What is a Vector Database?\n",
        "\n",
        "A **vector database** is designed to store and search embeddings efficiently.\n",
        "\n",
        "| Regular Database | Vector Database |\n",
        "|-----------------|----------------|\n",
        "| Stores text, numbers | Stores vectors (embeddings) |\n",
        "| Exact matching | Similarity matching |\n",
        "| SQL queries | Vector similarity search |\n",
        "\n",
        "### üìù Why Use a Vector Database?\n",
        "\n",
        "- When you have **thousands/millions** of documents\n",
        "- Comparing every vector is **too slow**\n",
        "- Vector DBs use special algorithms for **fast search**\n",
        "- They **persist data** between sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tjxSO1UJfET"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 14: Create a Chroma vector database\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "# Create a client\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "# Create embedding function (same model as before!)\n",
        "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "    model_name=\"all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# Create a collection\n",
        "collection = chroma_client.create_collection(\n",
        "    name=\"employee_handbook\",\n",
        "    embedding_function=embedding_func\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Chroma vector database created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNjceG-UJfET"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 15: Add chunks to the database\n",
        "collection.add(\n",
        "    documents=chunks,\n",
        "    ids=[f\"chunk_{i}\" for i in range(len(chunks))],\n",
        "    metadatas=[{\"source\": \"handbook\", \"chunk_id\": i} for i in range(len(chunks))]\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Added {collection.count()} chunks to the database\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5yCrv71JfET"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 16: Query the vector database\n",
        "query = \"How many days can I carry over?\"\n",
        "\n",
        "results = collection.query(\n",
        "    query_texts=[query],\n",
        "    n_results=2\n",
        ")\n",
        "\n",
        "print(f\"üîç Query: '{query}'\")\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"üìÑ Retrieved chunks:\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "for i, (doc, dist) in enumerate(zip(results['documents'][0], results['distances'][0]), 1):\n",
        "    similarity = 1 - dist  # Convert distance to similarity\n",
        "    print(f\"\\n[{i}] Similarity: {similarity:.3f}\")\n",
        "    print(f\"    {doc[:100]}...\" if len(doc) > 100 else f\"    {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6JiweJfJfET"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 7: Evaluation\n",
        "\n",
        "### üìù Two Things to Evaluate\n",
        "\n",
        "| Component | Question | If it fails... |\n",
        "|-----------|----------|---------------|\n",
        "| **Retrieval** | Did we find the right documents? | Wrong context ‚Üí Wrong answer |\n",
        "| **Generation** | Is the answer correct? | Even right context can give wrong answer |\n",
        "\n",
        "### üîë Important: Evaluate Retrieval FIRST!\n",
        "\n",
        "If retrieval fails, generation cannot succeed.\n",
        "\n",
        "### üìù Retrieval Metrics (from Week 5!)\n",
        "\n",
        "| Metric | Formula | Meaning |\n",
        "|--------|---------|--------|\n",
        "| **Precision@k** | Relevant in top-k √∑ k | What % of retrieved docs are relevant? |\n",
        "| **Recall@k** | Relevant in top-k √∑ Total relevant | What % of relevant docs did we find? |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hTg2qovJfET"
      },
      "outputs": [],
      "source": [
        "# üíª STEP 17: Evaluation function\n",
        "def evaluate_retrieval(retrieved_ids, relevant_ids):\n",
        "    \"\"\"\n",
        "    Calculate Precision and Recall for retrieval.\n",
        "    \"\"\"\n",
        "    retrieved = set(retrieved_ids)\n",
        "    relevant = set(relevant_ids)\n",
        "\n",
        "    true_positives = len(retrieved & relevant)\n",
        "\n",
        "    precision = true_positives / len(retrieved) if retrieved else 0\n",
        "    recall = true_positives / len(relevant) if relevant else 0\n",
        "\n",
        "    return {'precision': precision, 'recall': recall}\n",
        "\n",
        "# Example evaluation\n",
        "query = \"What is the parental leave policy?\"\n",
        "\n",
        "# Find which chunks are actually relevant (ground truth)\n",
        "relevant_ids = [i for i, chunk in enumerate(chunks) if 'parental' in chunk.lower()]\n",
        "\n",
        "# Get retrieved chunks\n",
        "results = collection.query(query_texts=[query], n_results=3)\n",
        "retrieved_ids = [int(id.split('_')[1]) for id in results['ids'][0]]\n",
        "\n",
        "print(f\"üîç Query: '{query}'\")\n",
        "print(f\"\\nüìä Relevant chunk IDs: {relevant_ids}\")\n",
        "print(f\"üìä Retrieved chunk IDs: {retrieved_ids}\")\n",
        "\n",
        "metrics = evaluate_retrieval(retrieved_ids, relevant_ids)\n",
        "print(f\"\\n‚úÖ Precision: {metrics['precision']:.2f}\")\n",
        "print(f\"‚úÖ Recall: {metrics['recall']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdRo1MejJfET"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 8: Common Failure Modes\n",
        "\n",
        "### üìù Understanding How RAG Can Fail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNq0Z7r4JfET"
      },
      "outputs": [],
      "source": [
        "# üíª Failure Mode 1: Information not in knowledge base\n",
        "query = \"What is the company's policy on pets in the office?\"\n",
        "\n",
        "results = collection.query(query_texts=[query], n_results=2)\n",
        "\n",
        "print(f\"üîç Query: '{query}'\")\n",
        "print(\"\\n‚ö†Ô∏è Retrieved documents:\")\n",
        "for doc in results['documents'][0]:\n",
        "    print(f\"   ‚Üí {doc[:60]}...\")\n",
        "\n",
        "print(\"\\n‚ùå PROBLEM: The knowledge base has NO information about pets!\")\n",
        "print(\"   A good RAG system should say 'I don't have that information.'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN2Kz6fNJfEU"
      },
      "outputs": [],
      "source": [
        "# üíª Failure Mode 2: Different terminology\n",
        "query = \"What's the PTO policy?\"  # PTO = American term\n",
        "\n",
        "results = collection.query(query_texts=[query], n_results=2)\n",
        "\n",
        "print(f\"üîç Query: '{query}'\")\n",
        "print(\"\\nüìÑ Retrieved:\")\n",
        "for doc, dist in zip(results['documents'][0], results['distances'][0]):\n",
        "    print(f\"   [{1-dist:.3f}] {doc[:50]}...\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è NOTE: Our documents use 'annual leave' (British), not 'PTO' (American).\")\n",
        "print(\"   Dense embeddings may still work, but it's not guaranteed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucKeCaTzJfEU"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 9: Summary\n",
        "\n",
        "### üîë Key Takeaways\n",
        "\n",
        "| Concept | What You Learned |\n",
        "|---------|------------------|\n",
        "| **RAG** | Retrieve ‚Üí Augment ‚Üí Generate |\n",
        "| **Why RAG** | Fixes LLM limitations (cutoff, hallucinations, no private data) |\n",
        "| **Dense embeddings** | Capture semantic meaning (\"holiday\" ‚âà \"annual leave\") |\n",
        "| **Chunking** | Split documents for precise retrieval |\n",
        "| **Vector databases** | Fast similarity search at scale |\n",
        "| **Evaluation** | Check retrieval AND generation separately |\n",
        "\n",
        "### üìä Recommended Settings\n",
        "\n",
        "| Parameter | Starting Value | Notes |\n",
        "|-----------|---------------|-------|\n",
        "| Chunk size | 500-1000 chars | Adjust for your documents |\n",
        "| Chunk overlap | 50-100 chars | ~10% of chunk size |\n",
        "| Top-k | 3-5 | Balance coverage vs noise |\n",
        "| Embedding model | all-MiniLM-L6-v2 | Free, fast, good quality |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3M16j1VJfEU"
      },
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Lab Exercise: Build Your Own RAG System\n",
        "\n",
        "**Task:** Build a RAG system for product reviews.\n",
        "\n",
        "**Steps:**\n",
        "1. Create a Chroma collection\n",
        "2. Add the reviews\n",
        "3. Query the system\n",
        "4. Generate answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78RcfMW8JfEU"
      },
      "outputs": [],
      "source": [
        "# Sample reviews for the exercise\n",
        "sample_reviews = [\n",
        "    \"The pen writes smoothly and the ink quality is excellent. Great value.\",\n",
        "    \"Disappointed. The scissors broke after just one use.\",\n",
        "    \"These markers are perfect for art projects. Vibrant colours!\",\n",
        "    \"Overpriced. I found better alternatives at half the price.\",\n",
        "    \"The notebook paper is too thin. Ink bleeds through.\",\n",
        "    \"Love these coloured pencils! They blend beautifully.\",\n",
        "    \"Poor quality control. Two pens in the pack were dried out.\",\n",
        "    \"Fast delivery and excellent packaging. Exceeded expectations.\",\n",
        "    \"Not worth the money. The highlighters stopped working quickly.\",\n",
        "    \"Best art supplies I've ever purchased. Will buy again!\"\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(sample_reviews)} sample reviews\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KFV550GJfEU"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ YOUR CODE HERE - Try it yourself first!\n",
        "# Step 1: Create a new collection called \"reviews\"\n",
        "# review_collection = chroma_client.create_collection(...)\n",
        "\n",
        "# Step 2: Add the reviews\n",
        "# review_collection.add(...)\n",
        "\n",
        "# Step 3: Query for \"quality issues\"\n",
        "# results = review_collection.query(...)\n",
        "\n",
        "# Step 4: Print the results\n",
        "\n",
        "print(\"üíª Try implementing this yourself first, then check the solution below!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4mle_aaJfEU"
      },
      "source": [
        "### üìù Solution\n",
        "\n",
        "Once you've tried it yourself, check your solution against this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-Ubbu0GJfEU"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ SOLUTION - Step 1: Create a new collection\n",
        "review_collection = chroma_client.create_collection(\n",
        "    name=\"product_reviews\",\n",
        "    embedding_function=embedding_func\n",
        ")\n",
        "print(\"‚úÖ Step 1: Collection created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfEJxx3hJfEU"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ SOLUTION - Step 2: Add the reviews to the collection\n",
        "review_collection.add(\n",
        "    documents=sample_reviews,\n",
        "    ids=[f\"review_{i}\" for i in range(len(sample_reviews))],\n",
        "    metadatas=[{\"source\": \"art_supplies\", \"review_id\": i} for i in range(len(sample_reviews))]\n",
        ")\n",
        "print(f\"‚úÖ Step 2: Added {review_collection.count()} reviews to the collection!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmhdzN23JfEU"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ SOLUTION - Step 3: Query the collection\n",
        "test_queries = [\n",
        "    \"Find reviews about quality issues\",\n",
        "    \"What do people say about the price?\",\n",
        "    \"Are there positive reviews about pens?\"\n",
        "]\n",
        "\n",
        "print(\"‚úÖ Step 3: Querying the review collection\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "for query in test_queries:\n",
        "    results = review_collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=3\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüîç Query: '{query}'\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, (doc, dist) in enumerate(zip(results['documents'][0], results['distances'][0]), 1):\n",
        "        similarity = 1 - dist\n",
        "        print(f\"   [{i}] ({similarity:.3f}) {doc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMsCyGCNJfEU"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ SOLUTION - Step 4: Use the full RAG pipeline to generate answers\n",
        "# First, create embeddings for the reviews (for our rag_answer function)\n",
        "review_embeddings = embedding_model.encode(sample_reviews)\n",
        "\n",
        "print(\"‚úÖ Step 4: Full RAG pipeline with reviews\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "rag_queries = [\n",
        "    \"What are the main quality complaints?\",\n",
        "    \"Which products are recommended?\"\n",
        "]\n",
        "\n",
        "for query in rag_queries:\n",
        "    answer, sources = rag_answer(query, review_embeddings, sample_reviews, embedding_model, generator, top_k=3)\n",
        "\n",
        "    print(f\"\\n‚ùì Question: {query}\")\n",
        "    print(f\"üí¨ Answer: {answer}\")\n",
        "    print(\"üìö Sources:\")\n",
        "    for doc, score in sources:\n",
        "        print(f\"   [{score:.3f}] {doc[:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxFwAa42JfEU"
      },
      "source": [
        "### üîë What You Learned in This Lab\n",
        "\n",
        "1. **Creating a collection** - `chroma_client.create_collection()`\n",
        "2. **Adding documents** - `collection.add(documents=..., ids=..., metadatas=...)`\n",
        "3. **Querying** - `collection.query(query_texts=[...], n_results=k)`\n",
        "4. **Full RAG pipeline** - Combining retrieval with generation\n",
        "\n",
        "### ‚úÖ Self-Check Questions\n",
        "\n",
        "Can you answer these?\n",
        "\n",
        "1. Why do we need unique IDs for each document?\n",
        "2. What happens if you increase `n_results` to 5?\n",
        "3. What's the difference between the Chroma query and our `retrieve_documents` function?\n",
        "4. How would you add new reviews to an existing collection?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMKUrEvVJfEU"
      },
      "source": [
        "---\n",
        "\n",
        "## üìö References\n",
        "\n",
        "**Original RAG Paper:**\n",
        "- Lewis et al. (2020), \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"\n",
        "- https://arxiv.org/abs/2005.11401\n",
        "\n",
        "**Tools:**\n",
        "- Sentence-Transformers: https://www.sbert.net/\n",
        "- Chroma: https://www.trychroma.com/\n",
        "- LangChain: https://python.langchain.com/\n",
        "\n",
        "**Further Reading:**\n",
        "- \"Lost in the Middle\" (Liu et al., 2023)\n",
        "- RAGAS evaluation framework"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}